#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é˜¿é‡Œå›½é™…ä¸šåŠ¡æ™ºèƒ½å¤ç›˜å·¥å…·
Ali International Business Intelligence Analysis Tool

åŠŸèƒ½ï¼š
1. Excelæ•°æ®è¯»å–å’Œåˆ†æ
2. AIæ™ºèƒ½åˆ†ç±»ï¼ˆé€šä¹‰åƒé—®APIï¼‰
3. æ•°æ®å¯è§†åŒ–
4. æŠ¥å‘Šç”Ÿæˆ
5. æ™ºèƒ½æé†’
6. GUIç•Œé¢
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# seaborn ä½œä¸ºå¯é€‰ä¾èµ–
try:
    import seaborn as sns
    SEABORN_AVAILABLE = True
except ImportError:
    SEABORN_AVAILABLE = False
import json
import logging
import argparse
import os
import sys
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
import requests
import warnings
from pathlib import Path
# tkinter ä½œä¸ºå¯é€‰ä¾èµ–ï¼ˆç½‘é¡µç‰ˆä¸éœ€è¦ï¼‰
try:
    import tkinter as tk
    from tkinter import ttk, filedialog, messagebox, scrolledtext
    TKINTER_AVAILABLE = True
except ImportError:
    TKINTER_AVAILABLE = False
    # åˆ›å»ºå ä½ç¬¦ä»¥é¿å…é”™è¯¯
    tk = None
    ttk = None
    filedialog = None
    messagebox = None
    scrolledtext = None
import threading
from reportlab.lib.pagesizes import letter, A4
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.units import inch
from reportlab.lib import colors
from docx import Document
from docx.shared import Inches
import plotly.graph_objects as go
import plotly.express as px
from plotly.offline import plot
import folium
from folium import plugins

# === ä¸­æ–‡å­—ä½“æ”¯æŒ - ä½¿ç”¨FontPropertiesç›´æ¥æŒ‡å®šå­—ä½“æ–‡ä»¶ ===
import matplotlib
matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯
import matplotlib.pyplot as plt
from matplotlib import rcParams
import matplotlib.font_manager as fm
import os

# å…¨å±€å­—ä½“é…ç½® - ç›´æ¥ä½¿ç”¨å­—ä½“æ–‡ä»¶è·¯å¾„
CHINESE_FONT_PATH = r'C:\Windows\Fonts\simhei.ttf'  # é»‘ä½“
CHINESE_FONT_PROP = None

if os.path.exists(CHINESE_FONT_PATH):
    try:
        CHINESE_FONT_PROP = fm.FontProperties(fname=CHINESE_FONT_PATH)
        print(f"[OK] åŠ è½½ä¸­æ–‡å­—ä½“: {CHINESE_FONT_PROP.get_name()} from {CHINESE_FONT_PATH}")
    except Exception as e:
        print(f"[WARNING] åŠ è½½å­—ä½“å¤±è´¥: {e}")
        CHINESE_FONT_PROP = None

# åŒæ—¶è®¾ç½®rcParamsä½œä¸ºå¤‡ç”¨
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'SimSun']
plt.rcParams['axes.unicode_minus'] = False
plt.rcParams['font.family'] = 'sans-serif'

# =============================================

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings('ignore')

# è¾…åŠ©å‡½æ•°ï¼šä¸ºmatplotlibå›¾è¡¨è®¾ç½®ä¸­æ–‡å­—ä½“
def set_chinese_font_for_plot(ax, title=None, xlabel=None, ylabel=None):
    """ä¸ºmatplotlibå›¾è¡¨çš„æ ‡é¢˜ã€æ ‡ç­¾è®¾ç½®ä¸­æ–‡å­—ä½“"""
    if CHINESE_FONT_PROP:
        if title:
            ax.set_title(title, fontproperties=CHINESE_FONT_PROP, fontsize=16, fontweight='bold', pad=20, color='black')
        if xlabel:
            ax.set_xlabel(xlabel, fontproperties=CHINESE_FONT_PROP, fontsize=12, color='black')
        if ylabel:
            ax.set_ylabel(ylabel, fontproperties=CHINESE_FONT_PROP, fontsize=12, color='black')
        
        # è®¾ç½®åˆ»åº¦æ ‡ç­¾å­—ä½“
        for label in ax.get_xticklabels():
            label.set_fontproperties(CHINESE_FONT_PROP)
            label.set_color('black')
        for label in ax.get_yticklabels():
            label.set_fontproperties(CHINESE_FONT_PROP)
            label.set_color('black')
    else:
        if title:
            ax.set_title(title, fontsize=16, fontweight='bold', pad=20, color='black')
        if xlabel:
            ax.set_xlabel(xlabel, fontsize=12, color='black')
        if ylabel:
            ax.set_ylabel(ylabel, fontsize=12, color='black')

class AliBusinessAnalyzer:
    """é˜¿é‡Œå›½é™…ä¸šåŠ¡æ™ºèƒ½å¤ç›˜å·¥å…·ä¸»ç±»"""
    
    def __init__(self, config_file: str = "config.json"):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        self.config = self._load_config(config_file)
        self.data = None
        self.analysis_results = {}
        self.setup_logging()
        
        # æ ‡å‡†åˆ—åï¼ˆå®Œå…¨åŒ¹é…Excelæ¨¡æ¿ï¼‰
        self.standard_columns = [
            'è¯¢ç›˜æ—¶é—´', 'å’¨è¯¢æ–¹å¼', 'è·Ÿè¿›ç­‰çº§', 'å®¢æˆ·åç§°', 'å®¢æˆ·å±‚çº§', 
            'æ‰€å±å¤§æ´²', 'å›½å®¶', 'è¯¢ä»·äº§å“', 'äº§å“ID', 'è·Ÿè¿›äºº', 
            'å¤‡æ³¨ (å¤±å•åŸå› +è·Ÿè¿›æœºä¼šç‚¹)', 'æœ€åè·Ÿè¿›æ—¶é—´'
        ]
        
        # åˆå§‹åŒ–å›¾è¡¨æ ·å¼
        self._setup_plot_style()
        
    def _load_config(self, config_file: str) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        default_config = {
            "api_key": "sk-9c3866e9c45b4e5ea89faa1796fe78ff",
            "api_url": "https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation",
            "data_path": "./data",
            "output_path": "./output",
            "log_level": "INFO",
            "max_retries": 3,
            "timeout": 30,
            "ai_model": "qwen-turbo",
            "classification_rules": {
                "A": "ç²¾å‡†è¯¢ç›˜ï¼šå®¢æˆ·æ˜ç¡®æŒ‡å‡ºäº§å“éœ€æ±‚ï¼ŒåŒ…å«å„ç§ä¿¡æ¯ï¼ˆæ•°é‡ã€è¿è¾“/æ”¯ä»˜è¦æ±‚ã€å…¬å¸ä¿¡æ¯ç­‰ï¼‰",
                "B": "æ™®é€šè¯¢ç›˜ï¼šå¹¿æ’’ç½‘è¯¢ç›˜ï¼Œå†…å®¹å¹¿æ³›ï¼Œåªæ˜¯è¯¢ä»·æˆ–å‘å¯¹äº§å“æ„Ÿå…´è¶£ï¼Œæˆ–ä¿¡æ¯æœªè¯»ï¼Œéœ€è¦ç»§ç»­è·Ÿè¿›äº†",
                "C": "ä¸ªäººä¹°å®¶/ä¸åŒ¹é…è¯¢ç›˜/åƒåœ¾è¯¢ç›˜",
                "X": "å·²ä¸‹æ ·å“å•/å¤§è´§å®¢æˆ·ï¼ŒæŒç»­è·Ÿè¿›"
            }
        }
        
        if os.path.exists(config_file):
            try:
                with open(config_file, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                # åˆå¹¶é»˜è®¤é…ç½®
                for key, value in default_config.items():
                    if key not in config:
                        config[key] = value
                return config
            except Exception as e:
                print(f"é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {e}")
                return default_config
        else:
            # åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶
            with open(config_file, 'w', encoding='utf-8') as f:
                json.dump(default_config, f, ensure_ascii=False, indent=2)
            return default_config
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_level = getattr(logging, self.config.get('log_level', 'INFO'))
        
        # è®¾ç½®UTF-8ç¼–ç çš„StreamHandler
        import sys
        stream_handler = logging.StreamHandler(sys.stdout)
        stream_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
        
        logging.basicConfig(
            level=log_level,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('ali_business_analyzer.log', encoding='utf-8'),
                stream_handler
            ]
        )
        self.logger = logging.getLogger(__name__)
        self.logger.info("é˜¿é‡Œå›½é™…ä¸šåŠ¡æ™ºèƒ½å¤ç›˜å·¥å…·å¯åŠ¨")
    
    def _setup_plot_style(self):
        """è®¾ç½®å›¾è¡¨æ ·å¼"""
        try:
            if SEABORN_AVAILABLE:
                sns.set_style("whitegrid")
            # å°è¯•ä½¿ç”¨ seaborn æ ·å¼ï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨é»˜è®¤æ ·å¼
            try:
                plt.style.use('seaborn-v0_8')
            except (OSError, KeyError):
                try:
                    plt.style.use('seaborn')
                except (OSError, KeyError):
                    # å¦‚æœéƒ½ä¸è¡Œï¼Œä½¿ç”¨é»˜è®¤æ ·å¼
                    pass
        except Exception as e:
            # å¦‚æœè®¾ç½®æ ·å¼å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ ·å¼ï¼Œä¸å½±å“ç¨‹åºè¿è¡Œ
            pass
        
    def read_excel(self, file_path: str, sheet_name: str = None) -> pd.DataFrame:
        """è¯»å–Excelæ–‡ä»¶ - æ”¯æŒå¤šä¸ªå·¥ä½œè¡¨"""
        try:
            self.logger.info(f"æ­£åœ¨è¯»å–Excelæ–‡ä»¶: {file_path}")
            
            # å°è¯•è¯»å–Excelæ–‡ä»¶
            if file_path.endswith('.xlsx') or file_path.endswith('.xls'):
                # è¯»å–æ‰€æœ‰å·¥ä½œè¡¨
                excel_file = pd.ExcelFile(file_path)
                sheet_names = excel_file.sheet_names
                self.logger.info(f"å‘ç° {len(sheet_names)} ä¸ªå·¥ä½œè¡¨: {sheet_names}")
                
                # å¦‚æœæŒ‡å®šäº†å·¥ä½œè¡¨ï¼Œåªè¯»å–æŒ‡å®šçš„
                if sheet_name:
                    if sheet_name not in sheet_names:
                        raise ValueError(f"å·¥ä½œè¡¨ '{sheet_name}' ä¸å­˜åœ¨")
                    df = pd.read_excel(file_path, sheet_name=sheet_name)
                    self.logger.info(f"è¯»å–å·¥ä½œè¡¨: {sheet_name}")
                else:
                    # è¯»å–æ‰€æœ‰å·¥ä½œè¡¨å¹¶åˆå¹¶
                    all_dfs = []
                    for sheet in sheet_names:
                        try:
                            df_sheet = pd.read_excel(file_path, sheet_name=sheet)
                            # æ·»åŠ å·¥ä½œè¡¨åç§°åˆ—ï¼ˆå¯é€‰ï¼‰
                            df_sheet['_æ¥æºå·¥ä½œè¡¨'] = sheet
                            all_dfs.append(df_sheet)
                            self.logger.info(f"ä»å·¥ä½œè¡¨ '{sheet}' è¯»å–äº† {len(df_sheet)} æ¡è®°å½•")
                        except Exception as e:
                            self.logger.error(f"è¯»å–å·¥ä½œè¡¨ '{sheet}' å¤±è´¥: {e}")
                            continue
                    
                    if not all_dfs:
                        raise ValueError("æ²¡æœ‰æˆåŠŸè¯»å–ä»»ä½•å·¥ä½œè¡¨")
                    
                    # åˆå¹¶æ‰€æœ‰æ•°æ®
                    df = pd.concat(all_dfs, ignore_index=True)
                    self.logger.info(f"åˆå¹¶åå…± {len(df)} æ¡è®°å½•")
                    
            else:
                raise ValueError("æ–‡ä»¶æ ¼å¼ä¸æ”¯æŒï¼Œè¯·ä½¿ç”¨Excelæ–‡ä»¶(.xlsxæˆ–.xls)")
            
            # æ ‡å‡†åŒ–åˆ—å
            df = self._standardize_columns(df)
            
            # æ•°æ®æ¸…æ´—
            df = self._clean_data(df)
            
            # è½¬æ¢æ—¥æœŸæ ¼å¼ï¼ˆå»æ‰æ—¶é—´éƒ¨åˆ†ï¼‰
            df = self._convert_date_format(df)
            
            self.data = df
            self.logger.info(f"æˆåŠŸè¯»å– {len(df)} æ¡è®°å½•")
            return df
            
        except Exception as e:
            self.logger.error(f"è¯»å–Excelæ–‡ä»¶å¤±è´¥: {e}")
            raise
    
    def _convert_date_format(self, df: pd.DataFrame) -> pd.DataFrame:
        """è½¬æ¢æ—¥æœŸæ ¼å¼ï¼Œåªä¿ç•™æ—¥æœŸï¼Œä¸åŒ…å«æ—¶é—´"""
        date_columns = ['è¯¢ç›˜æ—¶é—´', 'æœ€åè·Ÿè¿›æ—¶é—´']
        
        for col in date_columns:
            if col in df.columns:
                # å°è¯•è½¬æ¢æ—¥æœŸæ ¼å¼
                try:
                    # å…ˆè½¬æ¢ä¸ºdatetime
                    df[col] = pd.to_datetime(df[col], errors='coerce')
                    # åªå¯¹æœ‰æ•ˆçš„datetimeè¿›è¡Œæ ¼å¼åŒ–
                    mask = df[col].notna()
                    if mask.any():
                        df.loc[mask, col] = df.loc[mask, col].dt.strftime('%Y/%m/%d')
                    # ä¿æŒæ— æ•ˆæ—¥æœŸä¸ºç©ºå­—ç¬¦ä¸²è€Œä¸æ˜¯NaN
                    df[col] = df[col].fillna('')
                except Exception as e:
                    self.logger.warning(f"æ—¥æœŸåˆ— '{col}' è½¬æ¢å¤±è´¥: {e}")
        
        return df
    
    def _standardize_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ ‡å‡†åŒ–åˆ—å"""
        column_mapping = {}
        used_standard_columns = set()
        
        # åˆ›å»ºåˆ—åæ˜ å°„
        for col in df.columns:
            col_lower = str(col).lower().strip()
            
            # åŒ¹é…æ ‡å‡†åˆ—å
            if any(keyword in col_lower for keyword in ['è¯¢ç›˜', 'æ—¶é—´', 'inquiry', 'time']) and 'è¯¢ç›˜æ—¶é—´' not in used_standard_columns:
                column_mapping[col] = 'è¯¢ç›˜æ—¶é—´'
                used_standard_columns.add('è¯¢ç›˜æ—¶é—´')
            elif any(keyword in col_lower for keyword in ['å’¨è¯¢', 'æ–¹å¼', 'contact', 'method']) and 'å’¨è¯¢æ–¹å¼' not in used_standard_columns:
                column_mapping[col] = 'å’¨è¯¢æ–¹å¼'
                used_standard_columns.add('å’¨è¯¢æ–¹å¼')
            elif any(keyword in col_lower for keyword in ['è·Ÿè¿›', 'ç­‰çº§', 'level', 'grade']) and 'è·Ÿè¿›ç­‰çº§' not in used_standard_columns:
                column_mapping[col] = 'è·Ÿè¿›ç­‰çº§'
                used_standard_columns.add('è·Ÿè¿›ç­‰çº§')
            elif any(keyword in col_lower for keyword in ['å®¢æˆ·', 'åç§°', 'customer', 'name']) and 'å®¢æˆ·åç§°' not in used_standard_columns:
                column_mapping[col] = 'å®¢æˆ·åç§°'
                used_standard_columns.add('å®¢æˆ·åç§°')
            elif any(keyword in col_lower for keyword in ['å±‚çº§', 'tier', 'category']) and 'å®¢æˆ·å±‚çº§' not in used_standard_columns:
                column_mapping[col] = 'å®¢æˆ·å±‚çº§'
                used_standard_columns.add('å®¢æˆ·å±‚çº§')
            elif any(keyword in col_lower for keyword in ['å¤§æ´²', 'continent']) and 'æ‰€å±å¤§æ´²' not in used_standard_columns:
                column_mapping[col] = 'æ‰€å±å¤§æ´²'
                used_standard_columns.add('æ‰€å±å¤§æ´²')
            elif any(keyword in col_lower for keyword in ['å›½å®¶', 'country', 'nation']) and 'å›½å®¶' not in used_standard_columns:
                column_mapping[col] = 'å›½å®¶'
                used_standard_columns.add('å›½å®¶')
            elif any(keyword in col_lower for keyword in ['äº§å“', 'product', 'è¯¢ä»·']) and 'è¯¢ä»·äº§å“' not in used_standard_columns:
                column_mapping[col] = 'è¯¢ä»·äº§å“'
                used_standard_columns.add('è¯¢ä»·äº§å“')
            elif any(keyword in col_lower for keyword in ['id', 'äº§å“id']) and 'äº§å“ID' not in used_standard_columns:
                column_mapping[col] = 'äº§å“ID'
                used_standard_columns.add('äº§å“ID')
            elif any(keyword in col_lower for keyword in ['è·Ÿè¿›äºº', 'follower', 'handler']) and 'è·Ÿè¿›äºº' not in used_standard_columns:
                column_mapping[col] = 'è·Ÿè¿›äºº'
                used_standard_columns.add('è·Ÿè¿›äºº')
            elif any(keyword in col_lower for keyword in ['å¤‡æ³¨', 'remark', 'note']) and 'å¤‡æ³¨ (å¤±å•åŸå› +è·Ÿè¿›æœºä¼šç‚¹)' not in used_standard_columns:
                column_mapping[col] = 'å¤‡æ³¨ (å¤±å•åŸå› +è·Ÿè¿›æœºä¼šç‚¹)'
                used_standard_columns.add('å¤‡æ³¨ (å¤±å•åŸå› +è·Ÿè¿›æœºä¼šç‚¹)')
            elif any(keyword in col_lower for keyword in ['æœ€å', 'last', 'follow']) and 'æœ€åè·Ÿè¿›æ—¶é—´' not in used_standard_columns:
                column_mapping[col] = 'æœ€åè·Ÿè¿›æ—¶é—´'
                used_standard_columns.add('æœ€åè·Ÿè¿›æ—¶é—´')
        
        # é‡å‘½ååˆ—
        df = df.rename(columns=column_mapping)
        
        # æ·»åŠ ç¼ºå¤±çš„åˆ—
        for col in self.standard_columns:
            if col not in df.columns:
                df[col] = None
        
        return df
    
    def _clean_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ•°æ®æ¸…æ´—"""
        original_count = len(df)
        self.logger.info(f"æ¸…æ´—å‰æ•°æ®é‡: {original_count}")
        
        # å¤„ç†æ—¶é—´åˆ—
        time_columns = ['è¯¢ç›˜æ—¶é—´', 'æœ€åè·Ÿè¿›æ—¶é—´']
        for col in time_columns:
            if col in df.columns:
                try:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯Excelæ—¥æœŸåºåˆ—å·ï¼ˆæ•°å€¼ç±»å‹ï¼‰
                    if df[col].dtype in ['float64', 'int64']:
                        # Excelæ—¥æœŸä»1900-01-01å¼€å§‹ï¼Œåºåˆ—å·0å¯¹åº”1900-01-01
                        # éœ€è¦å‡å»60ä»¥ä¿®æ­£Excelçš„æ—¥æœŸèµ·å§‹é”™è¯¯ï¼ˆExcelè®¤ä¸º1900æ˜¯é—°å¹´ï¼‰
                        dates = df[col].dropna()
                        if len(dates) > 0:
                            # å°è¯•è½¬æ¢ä¸ºæ—¥æœŸ
                            try:
                                # å¦‚æœæ˜¯åˆç†çš„æ—¥æœŸåºåˆ—å·ï¼ˆå¤§äº25569è¡¨ç¤ºExcelæ—¥æœŸï¼Œå°äº25569å¯èƒ½æ˜¯æ–‡æœ¬ï¼‰
                                if dates.min() > 25568:
                                    # Excelæ—¥æœŸåºåˆ—å·
                                    df[col] = df[col].apply(lambda x: pd.to_datetime('1899-12-30') + pd.Timedelta(days=x) if pd.notna(x) and x > 0 else None)
                                else:
                                    # å¸¸è§„æ—¥æœŸè½¬æ¢
                                    df[col] = pd.to_datetime(df[col], errors='coerce')
                            except:
                                df[col] = pd.to_datetime(df[col], errors='coerce')
                        else:
                            df[col] = pd.to_datetime(df[col], errors='coerce')
                    else:
                        # å…¶ä»–ç±»å‹ï¼Œæ­£å¸¸è½¬æ¢
                        df[col] = pd.to_datetime(df[col], errors='coerce')
                except Exception as e:
                    self.logger.warning(f"å¤„ç†æ—¶é—´åˆ— '{col}' æ—¶å‡ºé”™: {e}")
        
        # åªæ›¿æ¢æ•°å­—ç±»å‹ä¸ºNaNçš„ç©ºå€¼
        df = df.fillna('')
        
        # å»é™¤å®Œå…¨é‡å¤çš„è¡Œï¼ˆåŸºäºæ‰€æœ‰åˆ—ï¼‰
        df = df.drop_duplicates()
        
        final_count = len(df)
        self.logger.info(f"æ¸…æ´—åæ•°æ®é‡: {final_count}")
        
        return df
    
    def ai_classify_customer(self, customer_data: Dict) -> Dict:
        """ä½¿ç”¨AIå¯¹å®¢æˆ·è¿›è¡Œåˆ†ç±»"""
        try:
            # æ„å»ºæç¤ºè¯
            prompt = self._build_classification_prompt(customer_data)
            
            # è°ƒç”¨é€šä¹‰åƒé—®API
            response = self._call_qwen_api(prompt)
            
            # è§£æå“åº”
            result = self._parse_classification_response(response)
            
            return result
            
        except Exception as e:
            self.logger.error(f"AIåˆ†ç±»å¤±è´¥: {e}")
            return {
                'classification': 'C',
                'intent': 'æœªçŸ¥',
                'suggestion': 'éœ€è¦è¿›ä¸€æ­¥äº†è§£å®¢æˆ·éœ€æ±‚'
            }
    
    def _build_classification_prompt(self, customer_data: Dict) -> str:
        """æ„å»ºåˆ†ç±»æç¤ºè¯"""
        prompt = f"""
è¯·æ ¹æ®ä»¥ä¸‹å®¢æˆ·ä¿¡æ¯è¿›è¡Œæ™ºèƒ½åˆ†ç±»ï¼š

å®¢æˆ·ä¿¡æ¯ï¼š
- å®¢æˆ·åç§°ï¼š{customer_data.get('å®¢æˆ·åç§°', 'æœªçŸ¥')}
- å’¨è¯¢æ–¹å¼ï¼š{customer_data.get('å’¨è¯¢æ–¹å¼', 'æœªçŸ¥')}
- è¯¢ä»·äº§å“ï¼š{customer_data.get('è¯¢ä»·äº§å“', 'æœªçŸ¥')}
- æ‰€å±å¤§æ´²ï¼š{customer_data.get('æ‰€å±å¤§æ´²', 'æœªçŸ¥')}
- å›½å®¶ï¼š{customer_data.get('å›½å®¶', 'æœªçŸ¥')}
- å¤‡æ³¨ï¼š{customer_data.get('å¤‡æ³¨ (å¤±å•åŸå› +è·Ÿè¿›æœºä¼šç‚¹)', 'æ— ')}

è¯·æŒ‰ç…§ä»¥ä¸‹è§„åˆ™è¿›è¡Œåˆ†ç±»ï¼š

Aç±»ï¼šç²¾å‡†è¯¢ç›˜ - å®¢æˆ·æ˜ç¡®æŒ‡å‡ºäº§å“éœ€æ±‚ï¼ŒåŒ…å«å„ç§ä¿¡æ¯ï¼ˆæ•°é‡ã€è¿è¾“/æ”¯ä»˜è¦æ±‚ã€å…¬å¸ä¿¡æ¯ç­‰ï¼‰
Bç±»ï¼šæ™®é€šè¯¢ç›˜ - å¹¿æ’’ç½‘è¯¢ç›˜ï¼Œå†…å®¹å¹¿æ³›ï¼Œåªæ˜¯è¯¢ä»·æˆ–å‘å¯¹äº§å“æ„Ÿå…´è¶£ï¼Œæˆ–ä¿¡æ¯æœªè¯»ï¼Œéœ€è¦ç»§ç»­è·Ÿè¿›äº†
Cç±»ï¼šä¸ªäººä¹°å®¶/ä¸åŒ¹é…è¯¢ç›˜/åƒåœ¾è¯¢ç›˜
Xç±»ï¼šå·²ä¸‹æ ·å“å•/å¤§è´§å®¢æˆ·ï¼ŒæŒç»­è·Ÿè¿›

åŒæ—¶è¯·åˆ†æï¼š
1. å®¢æˆ·æ„å›¾ï¼ˆå¦‚ï¼šé‡‡è´­ã€æ ·å“ã€ä»·æ ¼å’¨è¯¢ã€æŠ€æœ¯å’¨è¯¢ç­‰ï¼‰
2. è·Ÿè¿›å»ºè®®ï¼ˆå…·ä½“çš„è¡ŒåŠ¨å»ºè®®ï¼‰

è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼š
{{
    "classification": "A/B/C/X",
    "intent": "å®¢æˆ·æ„å›¾",
    "suggestion": "è·Ÿè¿›å»ºè®®"
}}
"""
        return prompt
    
    def _call_qwen_api(self, prompt: str) -> str:
        """è°ƒç”¨é€šä¹‰åƒé—®API"""
        headers = {
            'Authorization': f'Bearer {self.config["api_key"]}',
            'Content-Type': 'application/json'
        }
        
        data = {
            'model': self.config['ai_model'],
            'input': {
                'messages': [
                    {
                        'role': 'user',
                        'content': prompt
                    }
                ]
            },
            'parameters': {
                'temperature': 0.7,
                'max_tokens': 1000
            }
        }
        
        try:
            response = requests.post(
                self.config['api_url'],
                headers=headers,
                json=data,
                timeout=self.config['timeout']
            )
            
            if response.status_code == 200:
                result = response.json()
                return result['output']['text']
            else:
                self.logger.error(f"APIè°ƒç”¨å¤±è´¥: {response.status_code}")
                return ""
                
        except Exception as e:
            self.logger.error(f"APIè°ƒç”¨å¼‚å¸¸: {e}")
            return ""
    
    def _parse_classification_response(self, response: str) -> Dict:
        """è§£æAIå“åº”"""
        try:
            # å°è¯•è§£æJSON
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                json_str = json_match.group()
                return json.loads(json_str)
            else:
                # å¦‚æœæ— æ³•è§£æJSONï¼Œä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–ä¿¡æ¯
                classification = 'C'
                intent = 'æœªçŸ¥'
                suggestion = 'éœ€è¦è¿›ä¸€æ­¥äº†è§£å®¢æˆ·éœ€æ±‚'
                
                if 'A' in response:
                    classification = 'A'
                elif 'B' in response:
                    classification = 'B'
                elif 'X' in response:
                    classification = 'X'
                
                return {
                    'classification': classification,
                    'intent': intent,
                    'suggestion': suggestion
                }
                
        except Exception as e:
            self.logger.error(f"è§£æAIå“åº”å¤±è´¥: {e}")
            return {
                'classification': 'C',
                'intent': 'æœªçŸ¥',
                'suggestion': 'éœ€è¦è¿›ä¸€æ­¥äº†è§£å®¢æˆ·éœ€æ±‚'
            }
    
    def analyze_data(self) -> Dict:
        """æ•°æ®åˆ†æ"""
        if self.data is None:
            raise ValueError("è¯·å…ˆè¯»å–æ•°æ®")
        
        self.logger.info("å¼€å§‹æ•°æ®åˆ†æ")
        
        analysis_results = {}
        
        # 1. åŸºæœ¬ç»Ÿè®¡
        analysis_results['basic_stats'] = {
            'total_customers': len(self.data),
            'total_inquiries': len(self.data),
            'date_range': {
                'start': self.data['è¯¢ç›˜æ—¶é—´'].min() if 'è¯¢ç›˜æ—¶é—´' in self.data.columns else None,
                'end': self.data['è¯¢ç›˜æ—¶é—´'].max() if 'è¯¢ç›˜æ—¶é—´' in self.data.columns else None
            }
        }
        
        # 2. åœ°åŒºåˆ†æ
        if 'æ‰€å±å¤§æ´²' in self.data.columns:
            analysis_results['continent_analysis'] = self.data['æ‰€å±å¤§æ´²'].value_counts().to_dict()
        
        if 'å›½å®¶' in self.data.columns:
            analysis_results['country_analysis'] = self.data['å›½å®¶'].value_counts().head(10).to_dict()
        
        # 3. äº§å“åˆ†æ
        if 'è¯¢ä»·äº§å“' in self.data.columns:
            analysis_results['product_analysis'] = self.data['è¯¢ä»·äº§å“'].value_counts().head(10).to_dict()
        
        # 4. è·Ÿè¿›ç­‰çº§åˆ†æ
        if 'è·Ÿè¿›ç­‰çº§' in self.data.columns:
            analysis_results['follow_up_analysis'] = self.data['è·Ÿè¿›ç­‰çº§'].value_counts().to_dict()
        
        # 5. æ—¶é—´è¶‹åŠ¿åˆ†æ
        if 'è¯¢ç›˜æ—¶é—´' in self.data.columns:
            try:
                # ç¡®ä¿æ—¶é—´æ˜¯datetimeç±»å‹
                time_series = pd.to_datetime(self.data['è¯¢ç›˜æ—¶é—´'], errors='coerce')
                # ç§»é™¤æ— æ•ˆæ—¥æœŸ
                valid_times = time_series.dropna()
                if len(valid_times) > 0:
                    daily_inquiries = valid_times.groupby(valid_times.dt.date).size()
                    analysis_results['daily_trend'] = daily_inquiries.to_dict()
                else:
                    analysis_results['daily_trend'] = {}
            except Exception as e:
                self.logger.warning(f"æ—¶é—´è¶‹åŠ¿åˆ†æå¤±è´¥: {e}")
                analysis_results['daily_trend'] = {}
        
        # 6. è·Ÿè¿›äººåˆ†æ
        if 'è·Ÿè¿›äºº' in self.data.columns:
            analysis_results['handler_analysis'] = self.data['è·Ÿè¿›äºº'].value_counts().to_dict()
        
        self.analysis_results = analysis_results
        self.logger.info("æ•°æ®åˆ†æå®Œæˆ")
        
        return analysis_results
    
    def generate_visualizations(self, output_dir: str = "./output"):
        """ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨"""
        if self.data is None:
            raise ValueError("è¯·å…ˆè¯»å–æ•°æ®")
        
        os.makedirs(output_dir, exist_ok=True)
        
        # 1. åœ°åŒºåˆ†å¸ƒå›¾ (Continent Distribution)
        if 'æ‰€å±å¤§æ´²' in self.data.columns:
            fig, ax = plt.subplots(figsize=(12, 8))
            continent_counts = self.data['æ‰€å±å¤§æ´²'].value_counts()
            
            colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7', '#DDA0DD']
            wedges, texts, autotexts = ax.pie(
                continent_counts.values, 
                labels=continent_counts.index, 
                autopct='%1.1f%%',
                colors=colors, 
                startangle=90,
                textprops={'fontsize': 12, 'color': 'black'}
            )
            
            # è®¾ç½®ä¸­æ–‡å­—ä½“å’Œé¢œè‰²
            for text in texts:
                if CHINESE_FONT_PROP:
                    text.set_fontproperties(CHINESE_FONT_PROP)
                text.set_color('black')
                text.set_fontsize(14)
                text.set_fontweight('bold')
            for autotext in autotexts:
                autotext.set_color('white')
                autotext.set_fontweight('bold')
                autotext.set_fontsize(10)
            
            # è®¾ç½®æ ‡é¢˜
            if CHINESE_FONT_PROP:
                ax.set_title('å®¢æˆ·åœ°åŒºåˆ†å¸ƒ (Customer Regional Distribution)', 
                           fontproperties=CHINESE_FONT_PROP, fontsize=16, fontweight='bold', pad=20, color='black')
            else:
                ax.set_title('å®¢æˆ·åœ°åŒºåˆ†å¸ƒ (Customer Regional Distribution)', 
                           fontsize=16, fontweight='bold', pad=20, color='black')
            
            plt.tight_layout()
            plt.savefig(f"{output_dir}/continent_distribution.png", dpi=300, bbox_inches='tight', pad_inches=0.2)
            plt.close()
            print(f"[OK] åœ°åŒºåˆ†å¸ƒå›¾å·²ç”Ÿæˆ")
        
        # 2. å›½å®¶åˆ†å¸ƒå›¾ (Country Distribution)
        if 'å›½å®¶' in self.data.columns:
            fig, ax = plt.subplots(figsize=(15, 8))
            country_counts = self.data['å›½å®¶'].value_counts().head(15)
            
            bars = ax.bar(range(len(country_counts)), country_counts.values, color='steelblue', alpha=0.8)
            ax.set_xticks(range(len(country_counts)))
            ax.set_xticklabels(country_counts.index, rotation=45, ha='right', fontsize=11)
            
            # ä½¿ç”¨è¾…åŠ©å‡½æ•°è®¾ç½®ä¸­æ–‡å­—ä½“
            set_chinese_font_for_plot(ax, 
                                    title='Top 15 å›½å®¶å®¢æˆ·åˆ†å¸ƒ (Top 15 Countries)',
                                    xlabel='å›½å®¶ (Country)',
                                    ylabel='å®¢æˆ·æ•°é‡ (Customer Count)')
            
            ax.grid(axis='y', alpha=0.3, linestyle='--')
            ax.tick_params(axis='y', labelcolor='black')
            
            # åœ¨æŸ±å­ä¸Šæ·»åŠ æ•°å€¼
            for i, bar in enumerate(bars):
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height,
                       f'{int(height)}',
                       ha='center', va='bottom', fontsize=9, color='black')
            
            plt.tight_layout()
            plt.savefig(f"{output_dir}/country_distribution.png", dpi=300, bbox_inches='tight', pad_inches=0.2)
            plt.close()
            print(f"[OK] å›½å®¶åˆ†å¸ƒå›¾å·²ç”Ÿæˆ")
        
        # 3. äº§å“çƒ­åº¦å›¾ (Product Popularity)
        if 'è¯¢ä»·äº§å“' in self.data.columns:
            fig, ax = plt.subplots(figsize=(15, 8))
            product_counts = self.data['è¯¢ä»·äº§å“'].value_counts().head(15)
            
            bars = ax.barh(range(len(product_counts)), product_counts.values, color='coral', alpha=0.8)
            ax.set_yticks(range(len(product_counts)))
            ax.set_yticklabels(product_counts.index, fontsize=11)
            
            # ä½¿ç”¨è¾…åŠ©å‡½æ•°è®¾ç½®ä¸­æ–‡å­—ä½“
            set_chinese_font_for_plot(ax,
                                    title='Top 15 çƒ­é—¨äº§å“ (Top 15 Popular Products)',
                                    xlabel='è¯¢ç›˜æ¬¡æ•° (Inquiry Count)',
                                    ylabel=None)
            
            ax.grid(axis='x', alpha=0.3, linestyle='--')
            ax.tick_params(axis='x', labelcolor='black')
            ax.invert_yaxis()  # è®©æœ€å¤§å€¼æ˜¾ç¤ºåœ¨æœ€ä¸Šé¢
            
            # åœ¨æŸ±å­ä¸Šæ·»åŠ æ•°å€¼
            for i, bar in enumerate(bars):
                width = bar.get_width()
                ax.text(width, bar.get_y() + bar.get_height()/2.,
                       f' {int(width)}',
                       ha='left', va='center', fontsize=9, color='black')
            
            plt.tight_layout()
            plt.savefig(f"{output_dir}/product_popularity.png", dpi=300, bbox_inches='tight', pad_inches=0.2)
            plt.close()
            print(f"[OK] äº§å“çƒ­åº¦å›¾å·²ç”Ÿæˆ")
        
        # 4. æ—¶é—´è¶‹åŠ¿å›¾ (Daily Inquiry Trend)
        if 'è¯¢ç›˜æ—¶é—´' in self.data.columns:
            fig, ax = plt.subplots(figsize=(15, 8))
            daily_inquiries = self.data.groupby(self.data['è¯¢ç›˜æ—¶é—´'].dt.date).size()
            
            line = ax.plot(daily_inquiries.index, daily_inquiries.values, 
                          marker='o', linewidth=2, color='green', alpha=0.7, label='æ¯æ—¥è¯¢ç›˜')[0]
            ax.fill_between(daily_inquiries.index, daily_inquiries.values, alpha=0.2, color='green')
            
            # ä½¿ç”¨è¾…åŠ©å‡½æ•°è®¾ç½®ä¸­æ–‡å­—ä½“
            set_chinese_font_for_plot(ax,
                                    title='æ¯æ—¥è¯¢ç›˜è¶‹åŠ¿ (Daily Inquiry Trend)',
                                    xlabel='æ—¥æœŸ (Date)',
                                    ylabel='è¯¢ç›˜æ•°é‡ (Inquiry Count)')
            
            ax.tick_params(axis='x', rotation=45, labelcolor='black')
            ax.tick_params(axis='y', labelcolor='black')
            ax.grid(True, alpha=0.3, linestyle='--')
            
            # è®¾ç½®å›¾ä¾‹å­—ä½“
            if CHINESE_FONT_PROP:
                ax.legend(prop=CHINESE_FONT_PROP)
            else:
                ax.legend(prop={'size': 10})
            
            plt.tight_layout()
            plt.savefig(f"{output_dir}/daily_trend.png", dpi=300, bbox_inches='tight', pad_inches=0.2)
            plt.close()
            print(f"[OK] æ—¶é—´è¶‹åŠ¿å›¾å·²ç”Ÿæˆ")
        
        # 5. è·Ÿè¿›ç­‰çº§åˆ†å¸ƒ (Follow-up Level Distribution)
        if 'è·Ÿè¿›ç­‰çº§' in self.data.columns:
            fig, ax = plt.subplots(figsize=(10, 8))
            follow_up_counts = self.data['è·Ÿè¿›ç­‰çº§'].value_counts()
            
            colors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99']
            wedges, texts, autotexts = ax.pie(
                follow_up_counts.values, 
                labels=follow_up_counts.index, 
                autopct='%1.1f%%',
                colors=colors, 
                startangle=90,
                textprops={'fontsize': 12, 'color': 'black'}
            )
            
            # è®¾ç½®ä¸­æ–‡å­—ä½“å’Œé¢œè‰²
            for text in texts:
                if CHINESE_FONT_PROP:
                    text.set_fontproperties(CHINESE_FONT_PROP)
                text.set_color('black')
                text.set_fontsize(14)
                text.set_fontweight('bold')
            for autotext in autotexts:
                autotext.set_color('white')
                autotext.set_fontweight('bold')
                autotext.set_fontsize(10)
            
            # è®¾ç½®æ ‡é¢˜
            if CHINESE_FONT_PROP:
                ax.set_title('è·Ÿè¿›ç­‰çº§åˆ†å¸ƒ (Follow-up Level Distribution)', 
                           fontproperties=CHINESE_FONT_PROP, fontsize=16, fontweight='bold', pad=20, color='black')
            else:
                ax.set_title('è·Ÿè¿›ç­‰çº§åˆ†å¸ƒ (Follow-up Level Distribution)', 
                           fontsize=16, fontweight='bold', pad=20, color='black')
            
            plt.tight_layout()
            plt.savefig(f"{output_dir}/follow_up_distribution.png", dpi=300, bbox_inches='tight', pad_inches=0.2)
            plt.close()
            print(f"[OK] è·Ÿè¿›ç­‰çº§åˆ†å¸ƒå›¾å·²ç”Ÿæˆ")
        
        self.logger.info(f"å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åˆ°: {output_dir}")
    
    def generate_report(self, output_file: str = None, force_reanalyze: bool = True, user_date_range: tuple = None) -> str:
        """ç”ŸæˆAIæ™ºèƒ½åˆ†ææŠ¥å‘Šï¼ˆçº¯æ–‡æœ¬æ ¼å¼ï¼‰
        
        Args:
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            force_reanalyze: æ˜¯å¦å¼ºåˆ¶é‡æ–°åˆ†ææ•°æ®ï¼ˆé»˜è®¤Trueï¼Œç¡®ä¿ä½¿ç”¨å½“å‰æ•°æ®ï¼‰
            user_date_range: ç”¨æˆ·é€‰æ‹©çš„æ—¶é—´èŒƒå›´ (start_date, end_date)ï¼Œç”¨äºåœ¨æŠ¥å‘Šä¸­æ˜¾ç¤º
        """
        # å¼ºåˆ¶é‡æ–°åˆ†æå½“å‰æ•°æ®ï¼Œç¡®ä¿ä½¿ç”¨è¿‡æ»¤åçš„æ•°æ®
        if force_reanalyze or not self.analysis_results:
            self.analyze_data()
        
        # åªç”ŸæˆTXTæ ¼å¼æŠ¥å‘Š
        if output_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            text_file = f"./output/analysis_report_{timestamp}.txt"
        else:
            # ç¡®ä¿æ˜¯.txtæ–‡ä»¶
            text_file = output_file.replace('.pdf', '.txt')
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_dir = os.path.dirname(text_file)
        if output_dir:
            os.makedirs(output_dir, exist_ok=True)
        
        # ç›´æ¥ç”ŸæˆAIæ™ºèƒ½åˆ†ææ–‡æœ¬æŠ¥å‘Š
        self._generate_text_report(text_file, user_date_range)
        
        self.logger.info(f"AIæ™ºèƒ½åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {text_file}")
        
        return text_file  # è¿”å›æ–‡æœ¬æ–‡ä»¶è·¯å¾„ä¾›GUIä½¿ç”¨
    
    def _call_qianwen_api(self, data_summary: str) -> str:
        """è°ƒç”¨é˜¿é‡Œåƒé—®APIè¿›è¡Œæ™ºèƒ½åˆ†æ"""
        try:
            # é˜¿é‡Œåƒé—®APIé…ç½®
            api_key = os.getenv('DASHSCOPE_API_KEY', 'sk-09641de5f87c432b8f81c115bb0ab18a')  # æœ‰æ•ˆçš„API Key
            api_url = 'https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation'
            
            # æ„å»ºæç¤ºè¯
            prompt = f"""ä½ æ˜¯ä¸€ä½èµ„æ·±çš„é˜¿é‡Œå›½é™…ç«™æ•°æ®åˆ†æä¸“å®¶ï¼Œæ‹¥æœ‰10å¹´ä»¥ä¸Šçš„è·¨å¢ƒç”µå•†æ•°æ®åˆ†æç»éªŒã€‚
è¯·åŸºäºä»¥ä¸‹è¯¢ç›˜æ•°æ®ï¼Œæä¾›ä¸€ä»½ä¸“ä¸šã€è¯¦ç»†ã€æœ‰æ´å¯ŸåŠ›çš„ä¸šåŠ¡åˆ†ææŠ¥å‘Šã€‚

ã€æ•°æ®æ¦‚å†µã€‘
{data_summary}

ã€åˆ†æè¦æ±‚ã€‘
1. æ·±åº¦åˆ†æå¸‚åœºè¶‹åŠ¿å’Œå®¢æˆ·è¡Œä¸ºæ¨¡å¼
2. è¯†åˆ«ä¸šåŠ¡ä¸­çš„å…³é”®é—®é¢˜å’Œé£é™©ç‚¹
3. æä¾›å…·ä½“å¯æ‰§è¡Œçš„ä¼˜åŒ–å»ºè®®
4. å¯¹æ¯”å†å²æ•°æ®ï¼ˆå¦‚æœæœ‰å‘¨æœŸæ€§æ•°æ®ï¼‰ï¼Œåˆ†æå¢é•¿æˆ–ä¸‹æ»‘åŸå› 
5. é¢„æµ‹æœªæ¥è¶‹åŠ¿ï¼Œæå‡ºæˆ˜ç•¥è§„åˆ’å»ºè®®

ã€æŠ¥å‘Šç»“æ„ã€‘
è¯·æŒ‰ç…§ä»¥ä¸‹ç»“æ„è¾“å‡ºï¼Œå†…å®¹è¦ä¸“ä¸šã€å…·ä½“ã€æœ‰æ•°æ®æ”¯æ’‘ï¼š

ä¸€ã€æ‰§è¡Œæ‘˜è¦ï¼ˆæ ¸å¿ƒæŒ‡æ ‡ã€æ•´ä½“è¯„ä»·ã€å…³é”®å‘ç°ï¼‰
äºŒã€å¸‚åœºåˆ†æï¼ˆåœ°åŸŸåˆ†å¸ƒã€å¸‚åœºæœºä¼šã€ç«äº‰æ€åŠ¿ï¼‰
ä¸‰ã€äº§å“åˆ†æï¼ˆçƒ­é—¨äº§å“ã€äº§å“ç»„åˆã€ä¼˜åŒ–æ–¹å‘ï¼‰
å››ã€å®¢æˆ·è´¨é‡åˆ†æï¼ˆå±‚çº§åˆ†å¸ƒã€è½¬åŒ–ç‡ã€å®¢æˆ·ä»·å€¼ï¼‰
äº”ã€æ—¶é—´è¶‹åŠ¿åˆ†æï¼ˆè¯¢ç›˜è¶‹åŠ¿ã€å‘¨æœŸæ€§ç‰¹å¾ã€å¢é•¿é©±åŠ¨å› ç´ ï¼‰
å…­ã€å›¢é˜Ÿç»©æ•ˆåˆ†æï¼ˆæˆå‘˜è¡¨ç°ã€åä½œæ•ˆç‡ã€åŸ¹è®­éœ€æ±‚ï¼‰
ä¸ƒã€é—®é¢˜è¯Šæ–­ä¸é£é™©é¢„è­¦ï¼ˆç°å­˜é—®é¢˜ã€æ½œåœ¨é£é™©ã€åº”å¯¹æ–¹æ¡ˆï¼‰
å…«ã€æˆ˜ç•¥è¡ŒåŠ¨å»ºè®®ï¼ˆçŸ­æœŸ1-2å‘¨ã€ä¸­æœŸ1-3æœˆã€é•¿æœŸ3-6æœˆï¼‰
ä¹ã€æ€»ç»“ä¸å±•æœ›ï¼ˆæ•´ä½“è¯„ä»·ã€å…³é”®æˆåŠŸå› ç´ ã€æœªæ¥æ–¹å‘ï¼‰

æ³¨æ„ï¼š
1. æ‰€æœ‰åˆ†æå¿…é¡»åŸºäºæä¾›çš„çœŸå®æ•°æ®
2. æ¯ä¸ªç»“è®ºéƒ½è¦æœ‰æ•°æ®æ”¯æ’‘
3. å»ºè®®è¦å…·ä½“å¯æ‰§è¡Œï¼Œä¸è¦æ³›æ³›è€Œè°ˆ
4. ä½¿ç”¨ä¸“ä¸šçš„è·¨å¢ƒç”µå•†æœ¯è¯­
5. ä¿æŒå®¢è§‚ã€ç†æ€§çš„åˆ†ææ€åº¦"""

            # APIè¯·æ±‚
            headers = {
                'Authorization': f'Bearer {api_key}',
                'Content-Type': 'application/json'
            }
            
            payload = {
                "model": "qwen-max",  # ä½¿ç”¨åƒé—®æœ€å¼ºæ¨¡å‹
                "input": {
                    "messages": [
                        {
                            "role": "system",
                            "content": "ä½ æ˜¯ä¸€ä½èµ„æ·±çš„é˜¿é‡Œå›½é™…ç«™æ•°æ®åˆ†æä¸“å®¶ï¼Œæ“…é•¿ä»æ•°æ®ä¸­æŒ–æ˜å•†ä¸šæ´å¯Ÿï¼Œæä¾›ä¸“ä¸šçš„ä¸šåŠ¡å»ºè®®ã€‚"
                        },
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ]
                },
                "parameters": {
                    "result_format": "message",
                    "max_tokens": 4000,  # å¢åŠ tokenæ•°é‡ä»¥è·å¾—æ›´è¯¦ç»†çš„åˆ†æ
                    "temperature": 0.7,  # ä¿æŒä¸€å®šåˆ›é€ æ€§
                    "top_p": 0.9
                }
            }
            
            print("[AIåˆ†æ] æ­£åœ¨è°ƒç”¨é˜¿é‡Œåƒé—®APIè¿›è¡Œæ™ºèƒ½åˆ†æ...")
            print("[AIåˆ†æ] æç¤º: APIè°ƒç”¨å¯èƒ½éœ€è¦30-90ç§’ï¼Œè¯·è€å¿ƒç­‰å¾…...")
            
            # å¢åŠ é‡è¯•æœºåˆ¶å’Œè¶…æ—¶è®¾ç½®
            max_retries = 2
            timeout = 120  # å¢åŠ åˆ°120ç§’
            
            for attempt in range(max_retries):
                try:
                    if attempt > 0:
                        print(f"[AIåˆ†æ] é‡è¯•ç¬¬ {attempt} æ¬¡...")
                    
                    response = requests.post(api_url, headers=headers, json=payload, timeout=timeout)
                    
                    if response.status_code == 200:
                        result = response.json()
                        if 'output' in result and 'choices' in result['output']:
                            ai_analysis = result['output']['choices'][0]['message']['content']
                            print("[AIåˆ†æ] [OK] AIåˆ†æå®Œæˆ")
                            return ai_analysis
                        else:
                            print(f"[AIåˆ†æ] [WARNING] APIè¿”å›æ ¼å¼å¼‚å¸¸: {result}")
                            if attempt < max_retries - 1:
                                continue
                            return None
                    else:
                        print(f"[AIåˆ†æ] [ERROR] APIè°ƒç”¨å¤±è´¥: {response.status_code}")
                        if attempt < max_retries - 1:
                            continue
                        return None
                        
                except requests.exceptions.Timeout:
                    print(f"[AIåˆ†æ] [WARNING] APIè°ƒç”¨è¶…æ—¶ (å°è¯• {attempt + 1}/{max_retries})")
                    if attempt < max_retries - 1:
                        print("[AIåˆ†æ] æ­£åœ¨é‡è¯•...")
                        continue
                    print("[AIåˆ†æ] [ERROR] å¤šæ¬¡å°è¯•åä»ç„¶è¶…æ—¶ï¼Œå°†ä½¿ç”¨å¤‡ç”¨æ¨¡æ¿")
                    return None
                    
                except requests.exceptions.RequestException as e:
                    print(f"[AIåˆ†æ] [ERROR] ç½‘ç»œè¯·æ±‚é”™è¯¯: {str(e)}")
                    if attempt < max_retries - 1:
                        continue
                    return None
            
            return None
                
        except Exception as e:
            print(f"[AIåˆ†æ] [ERROR] è°ƒç”¨åƒé—®APIå¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return None
    
    def _prepare_data_summary(self, user_date_range: tuple = None) -> str:
        """å‡†å¤‡æ•°æ®æ‘˜è¦ä¾›AIåˆ†æ
        
        Args:
            user_date_range: ç”¨æˆ·é€‰æ‹©çš„æ—¶é—´èŒƒå›´ (start_date, end_date)
        """
        if self.data is None or self.data.empty:
            return "æ— æ•°æ®"
        
        # æ ¼å¼åŒ–æ—¥æœŸ - ä¼˜å…ˆä½¿ç”¨ç”¨æˆ·é€‰æ‹©çš„æ—¶é—´èŒƒå›´
        if user_date_range:
            min_date = user_date_range[0]
            max_date = user_date_range[1]
            time_span = (pd.to_datetime(max_date) - pd.to_datetime(min_date)).days + 1
        else:
            # å¦‚æœæ²¡æœ‰ç”¨æˆ·é€‰æ‹©ï¼Œä½¿ç”¨æ•°æ®ä¸­çš„å®é™…æ—¥æœŸ
            min_date = pd.to_datetime(self.data['è¯¢ç›˜æ—¶é—´'].min()).strftime('%Y-%m-%d')
            max_date = pd.to_datetime(self.data['è¯¢ç›˜æ—¶é—´'].max()).strftime('%Y-%m-%d')
            time_span = (pd.to_datetime(max_date) - pd.to_datetime(min_date)).days + 1
        
        total_inquiries = len(self.data)
        total_customers = self.data['å®¢æˆ·åç§°'].nunique()
        total_countries = self.data['å›½å®¶'].nunique()
        
        # è®¡ç®—å„å±‚çº§è¯¢ç›˜
        level_a_count = len(self.data[self.data['è·Ÿè¿›ç­‰çº§'] == 'A'])
        level_b_count = len(self.data[self.data['è·Ÿè¿›ç­‰çº§'] == 'B'])
        level_c_count = len(self.data[self.data['è·Ÿè¿›ç­‰çº§'] == 'C'])
        level_x_count = len(self.data[self.data['è·Ÿè¿›ç­‰çº§'] == 'X'])
        
        # å›½å®¶åˆ†å¸ƒ
        country_dist = self.data['å›½å®¶'].value_counts().head(10)
        country_str = "\n".join([f"  {i+1}. {country}: {count}æ¡ ({count/total_inquiries*100:.1f}%)" 
                                 for i, (country, count) in enumerate(country_dist.items())])
        
        # äº§å“åˆ†å¸ƒ
        product_dist = self.data['è¯¢ä»·äº§å“'].value_counts().head(10)
        product_str = "\n".join([f"  {i+1}. {product}: {count}æ¬¡ ({count/total_inquiries*100:.1f}%)" 
                                 for i, (product, count) in enumerate(product_dist.items())])
        
        # å®¢æˆ·å±‚çº§åˆ†å¸ƒ
        if 'å®¢æˆ·å±‚çº§' in self.data.columns:
            level_dist = self.data['å®¢æˆ·å±‚çº§'].value_counts()
            level_str = "\n".join([f"  {level}: {level_dist.get(level, 0)}æ¡ ({level_dist.get(level, 0)/total_inquiries*100:.1f}%)" 
                                   for level in ['L4', 'L3', 'L2', 'L1', 'L0']])
        else:
            level_str = "  æ— å®¢æˆ·å±‚çº§æ•°æ®"
        
        # å’¨è¯¢æ–¹å¼åˆ†å¸ƒ
        method_dist = self.data['å’¨è¯¢æ–¹å¼'].value_counts()
        method_str = "\n".join([f"  {method}: {count}æ¡ ({count/total_inquiries*100:.1f}%)" 
                                for method, count in method_dist.items()])
        
        # å›¢é˜Ÿç»©æ•ˆ
        handler_performance = self.data.groupby('è·Ÿè¿›äºº').agg({
            'å®¢æˆ·åç§°': 'count',
            'è·Ÿè¿›ç­‰çº§': lambda x: (x == 'A').sum()
        }).rename(columns={'å®¢æˆ·åç§°': 'è¯¢ç›˜æ•°', 'è·Ÿè¿›ç­‰çº§': 'Açº§æ•°'})
        handler_performance['Açº§å æ¯”'] = (handler_performance['Açº§æ•°'] / handler_performance['è¯¢ç›˜æ•°'] * 100).round(1)
        handler_performance = handler_performance.sort_values('è¯¢ç›˜æ•°', ascending=False)
        handler_str = "\n".join([f"  {handler}: {int(row['è¯¢ç›˜æ•°'])}æ¡è¯¢ç›˜, {int(row['Açº§æ•°'])}æ¡Açº§ ({row['Açº§å æ¯”']}%)" 
                                 for handler, row in handler_performance.iterrows()])
        
        # æ—¶é—´è¶‹åŠ¿
        self.data['æ—¥æœŸ'] = pd.to_datetime(self.data['è¯¢ç›˜æ—¶é—´']).dt.date
        daily_trend = self.data.groupby('æ—¥æœŸ').size()
        avg_daily = daily_trend.mean()
        max_day = daily_trend.idxmax()
        max_count = daily_trend.max()
        min_day = daily_trend.idxmin()
        min_count = daily_trend.min()
        
        # å‘¨åº¦å¢é•¿ç‡ï¼ˆå¦‚æœæ—¶é—´è·¨åº¦è¶³å¤Ÿï¼‰
        growth_rate_str = ""
        if time_span >= 14:
            self.data['å‘¨æ¬¡'] = pd.to_datetime(self.data['è¯¢ç›˜æ—¶é—´']).dt.isocalendar().week
            weekly_trend = self.data.groupby('å‘¨æ¬¡').size()
            if len(weekly_trend) >= 2:
                recent_week_avg = weekly_trend.iloc[-2:].mean()
                early_week_avg = weekly_trend.iloc[:2].mean() if len(weekly_trend) > 2 else recent_week_avg
                growth_rate = ((recent_week_avg - early_week_avg) / early_week_avg * 100) if early_week_avg > 0 else 0
                growth_rate_str = f"\nâ€¢ å‘¨åº¦å¢é•¿ç‡: {growth_rate:+.1f}% (è¿‘æœŸå‘¨å‡{recent_week_avg:.1f}æ¡ vs æ—©æœŸå‘¨å‡{early_week_avg:.1f}æ¡)"
        
        summary = f"""
æ—¶é—´èŒƒå›´: {min_date} è‡³ {max_date} (å…±{time_span}å¤©)

æ ¸å¿ƒæŒ‡æ ‡:
â€¢ æ€»è¯¢ç›˜æ•°: {total_inquiries}æ¡
â€¢ æ—¥å‡è¯¢ç›˜: {avg_daily:.1f}æ¡
â€¢ ç‹¬ç«‹å®¢æˆ·æ•°: {total_customers}ä¸ª
â€¢ è¦†ç›–å›½å®¶: {total_countries}ä¸ª
â€¢ Açº§è¯¢ç›˜: {level_a_count}æ¡ ({level_a_count/total_inquiries*100:.1f}%) - ç²¾å‡†é«˜ä»·å€¼
â€¢ Bçº§è¯¢ç›˜: {level_b_count}æ¡ ({level_b_count/total_inquiries*100:.1f}%) - æœ‰æ½œåŠ›
â€¢ Cçº§è¯¢ç›˜: {level_c_count}æ¡ ({level_c_count/total_inquiries*100:.1f}%) - éœ€åŸ¹è‚²
â€¢ Xçº§è¯¢ç›˜: {level_x_count}æ¡ ({level_x_count/total_inquiries*100:.1f}%) - æ— æ•ˆè¯¢ç›˜

TOP 10 å›½å®¶åˆ†å¸ƒ:
{country_str}

TOP 10 çƒ­é—¨äº§å“:
{product_str}

å®¢æˆ·å±‚çº§åˆ†å¸ƒ (L4æœ€é«˜ï¼ŒL0æœ€ä½):
{level_str}

å’¨è¯¢æ–¹å¼åˆ†å¸ƒ:
{method_str}

å›¢é˜Ÿæˆå‘˜ç»©æ•ˆ:
{handler_str}

æ—¶é—´è¶‹åŠ¿:
â€¢ æ—¥å‡è¯¢ç›˜: {avg_daily:.1f}æ¡
â€¢ å³°å€¼: {max_day} ({max_count}æ¡)
â€¢ ä½è°·: {min_day} ({min_count}æ¡){growth_rate_str}
"""
        return summary
    
    def _generate_text_report(self, output_file: str, user_date_range: tuple = None):
        """ç”ŸæˆAIé©±åŠ¨çš„ä¸“ä¸šè¯¦ç»†æ–‡æœ¬æ ¼å¼æŠ¥å‘Š
        
        Args:
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            user_date_range: ç”¨æˆ·é€‰æ‹©çš„æ—¶é—´èŒƒå›´ (start_date, end_date)
        """
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write("=" * 100 + "\n")
                f.write("                    é˜¿é‡Œå›½é™…ä¸šåŠ¡æ•°æ®åˆ†ææŠ¥å‘Š\n")
                f.write("             Ali International Business Data Analysis Report\n")
                f.write("                  (Powered by é˜¿é‡Œåƒé—® AI)\n")
                f.write("=" * 100 + "\n\n")
                
                if self.data is None or self.data.empty:
                    f.write("âš ï¸ æ•°æ®ä¸ºç©ºï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Š\n")
                    return
                
                # å‡†å¤‡æ•°æ®æ‘˜è¦
                print("\n" + "="*80)
                print("[æŠ¥å‘Šç”Ÿæˆ] æ­¥éª¤ 1/3: å‡†å¤‡æ•°æ®æ‘˜è¦...")
                data_summary = self._prepare_data_summary(user_date_range)
                print("[æŠ¥å‘Šç”Ÿæˆ] [OK] æ•°æ®æ‘˜è¦å‡†å¤‡å®Œæˆ")
                
                # è°ƒç”¨AIåˆ†æ
                print("[æŠ¥å‘Šç”Ÿæˆ] æ­¥éª¤ 2/3: è°ƒç”¨é˜¿é‡Œåƒé—®AIè¿›è¡Œæ™ºèƒ½åˆ†æ...")
                ai_analysis = self._call_qianwen_api(data_summary)
                
                # å†™å…¥æŠ¥å‘Š
                print("[æŠ¥å‘Šç”Ÿæˆ] æ­¥éª¤ 3/3: ç”ŸæˆæŠ¥å‘Šæ–‡ä»¶...")
                
                # æ ¼å¼åŒ–æ—¥æœŸ - ä¼˜å…ˆä½¿ç”¨ç”¨æˆ·é€‰æ‹©çš„æ—¶é—´èŒƒå›´
                if user_date_range:
                    min_date = user_date_range[0]
                    max_date = user_date_range[1]
                    time_span = (pd.to_datetime(max_date) - pd.to_datetime(min_date)).days + 1
                    print(f"[æŠ¥å‘Šç”Ÿæˆ] ä½¿ç”¨ç”¨æˆ·é€‰æ‹©çš„æ—¶é—´èŒƒå›´: {min_date} è‡³ {max_date}")
                else:
                    min_date = pd.to_datetime(self.data['è¯¢ç›˜æ—¶é—´'].min()).strftime('%Y-%m-%d')
                    max_date = pd.to_datetime(self.data['è¯¢ç›˜æ—¶é—´'].max()).strftime('%Y-%m-%d')
                    time_span = (pd.to_datetime(max_date) - pd.to_datetime(min_date)).days + 1
                    print(f"[æŠ¥å‘Šç”Ÿæˆ] ä½¿ç”¨æ•°æ®ä¸­çš„å®é™…æ—¶é—´èŒƒå›´: {min_date} è‡³ {max_date}")
                
                f.write(f"ğŸ“… åˆ†ææ—¶æ®µ: {min_date} è‡³ {max_date} (å…± {time_span} å¤©)\n")
                f.write(f"ğŸ“Š æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"ğŸ¤– åˆ†æå¼•æ“: é˜¿é‡Œåƒé—® (qwen-max)\n")
                f.write("\n" + "=" * 100 + "\n\n")
                
                if ai_analysis:
                    # ä½¿ç”¨AIç”Ÿæˆçš„åˆ†ææŠ¥å‘Š
                    f.write(ai_analysis)
                    f.write("\n\n")
                    print("[æŠ¥å‘Šç”Ÿæˆ] [OK] AIåˆ†ææŠ¥å‘Šå·²å†™å…¥")
                else:
                    # AIè°ƒç”¨å¤±è´¥æ—¶çš„å¤‡ç”¨æ–¹æ¡ˆ
                    f.write("[è­¦å‘Š] AIåˆ†ææœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œä½¿ç”¨å¤‡ç”¨åˆ†ææ¨¡æ¿\n\n")
                    print("[æŠ¥å‘Šç”Ÿæˆ] [WARNING] AIåˆ†æå¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ¨¡æ¿")
                    self._generate_fallback_report(f)
                
                # æ·»åŠ æ•°æ®é™„å½•
                f.write("\n" + "=" * 100 + "\n")
                f.write("ã€æ•°æ®é™„å½• DATA APPENDIXã€‘\n")
                f.write("=" * 100 + "\n\n")
                f.write(data_summary)
                f.write("\n")
                
                f.write("\n" + "=" * 100 + "\n")
                f.write("æŠ¥å‘Šç»“æŸ END OF REPORT\n")
                f.write("=" * 100 + "\n")
                
                print("[æŠ¥å‘Šç”Ÿæˆ] [OK] æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
                print("="*80 + "\n")
                
        except Exception as e:
            print(f"[æŠ¥å‘Šç”Ÿæˆ] [ERROR] ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Šå¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
    
    def _generate_fallback_report(self, f):
        """å¤‡ç”¨æŠ¥å‘Šç”Ÿæˆæ–¹æ¡ˆï¼ˆå½“AIä¸å¯ç”¨æ—¶ï¼‰"""
        f.write("æ³¨æ„ï¼šå½“å‰ä½¿ç”¨ç®€åŒ–æ¨¡æ¿ã€‚å»ºè®®é…ç½®é˜¿é‡Œåƒé—®APIä»¥è·å¾—æ›´æ™ºèƒ½çš„åˆ†æã€‚\n\n")
        f.write("ã€ç®€è¦åˆ†æã€‘\n\n")
        
        total_inquiries = len(self.data)
        level_a_count = len(self.data[self.data['è·Ÿè¿›ç­‰çº§'] == 'A'])
        
        f.write(f"æœ¬æœŸå…±æ”¶åˆ° {total_inquiries} æ¡è¯¢ç›˜ï¼Œå…¶ä¸­Açº§é«˜ä»·å€¼è¯¢ç›˜ {level_a_count} æ¡ã€‚\n")
        f.write("å»ºè®®é‡ç‚¹å…³æ³¨Açº§å’ŒBçº§å®¢æˆ·ï¼Œä¼˜åŒ–äº§å“æ¨å¹¿ç­–ç•¥ï¼ŒåŠ å¼ºå¸‚åœºæ‹“å±•ã€‚\n\n")
        f.write("è¯¦ç»†æ•°æ®è¯·æŸ¥çœ‹ä¸‹æ–¹æ•°æ®é™„å½•ã€‚\n")
    
    def _analyze_high_value_customers(self):
        """åˆ†æé«˜ä»·å€¼å®¢æˆ·"""
        # è¿™ä¸ªæ–¹æ³•å·²è¢«AIåˆ†ææ›¿ä»£ï¼Œä¿ç•™ä»¥é¿å…ç ´åå…¶ä»–ä»£ç 
        return []
    
    def _analyze_priority_customers(self):
        """åˆ†æéœ€è¦é‡ç‚¹è·Ÿè¿›çš„å®¢æˆ·"""
        # è¿™ä¸ªæ–¹æ³•å·²è¢«AIåˆ†ææ›¿ä»£ï¼Œä¿ç•™ä»¥é¿å…ç ´åå…¶ä»–ä»£ç 
        return []
    
    def _analyze_lost_customers(self):
        """åˆ†æå¤±å•åŸå› """
        # è¿™ä¸ªæ–¹æ³•å·²è¢«AIåˆ†ææ›¿ä»£ï¼Œä¿ç•™ä»¥é¿å…ç ´åå…¶ä»–ä»£ç 
        return {}
    
    def _generate_recommendations(self):
        """ç”Ÿæˆå‘å±•å»ºè®®"""
        # è¿™ä¸ªæ–¹æ³•å·²è¢«AIåˆ†ææ›¿ä»£ï¼Œä¿ç•™ä»¥é¿å…ç ´åå…¶ä»–ä»£ç 
        return []
    
    # åŸæœ‰çš„è¾…åŠ©æ–¹æ³•å·²è¢«ç§»é™¤ï¼Œæ–°æŠ¥å‘Šå®Œå…¨ç”±AIç”Ÿæˆ
    
    # =========================
    # æ™ºèƒ½æé†’
    # =========================
    
    def get_alerts(self) -> List[Dict]:
        """ç”Ÿæˆæ™ºèƒ½æé†’"""
        alerts = []
        
        if self.data is None:
            return alerts
        
        current_date = datetime.now()
        
        # æ£€æŸ¥é•¿æ—¶é—´æœªè·Ÿè¿›çš„å®¢æˆ·
        if 'æœ€åè·Ÿè¿›æ—¶é—´' in self.data.columns:
            overdue_customers = self.data[
                pd.notna(self.data['æœ€åè·Ÿè¿›æ—¶é—´'])
            ]
            
            for _, customer in overdue_customers.iterrows():
                last_follow_up = pd.to_datetime(customer['æœ€åè·Ÿè¿›æ—¶é—´'])
                days_since = (current_date - last_follow_up).days
                
                if days_since > 7:  # è¶…è¿‡7å¤©æœªè·Ÿè¿›
                    alerts.append({
                        'type': 'follow_up_overdue',
                        'priority': 'high' if days_since > 14 else 'medium',
                        'message': f"å®¢æˆ· {customer.get('å®¢æˆ·åç§°', 'æœªçŸ¥')} å·² {days_since} å¤©æœªè·Ÿè¿›",
                        'customer_name': customer.get('å®¢æˆ·åç§°', 'æœªçŸ¥'),
                        'days_overdue': days_since
                    })
        
        # æ£€æŸ¥é«˜ä»·å€¼å®¢æˆ·
        if 'è·Ÿè¿›ç­‰çº§' in self.data.columns:
            high_value_customers = self.data[
                self.data['è·Ÿè¿›ç­‰çº§'].isin(['A'])
            ]
            
            for _, customer in high_value_customers.iterrows():
                alerts.append({
                    'type': 'high_value_customer',
                    'priority': 'medium',
                    'message': f"é«˜ä»·å€¼å®¢æˆ· {customer.get('å®¢æˆ·åç§°', 'æœªçŸ¥')} éœ€è¦ç‰¹åˆ«å…³æ³¨",
                    'customer_name': customer.get('å®¢æˆ·åç§°', 'æœªçŸ¥'),
                    'level': customer.get('è·Ÿè¿›ç­‰çº§'),
                    'continent': customer.get('æ‰€å±å¤§æ´²', 'æœªçŸ¥')
                })
        
        return alerts[:10]  # åªè¿”å›å‰10ä¸ªæé†’
    def _analyze_high_value_customers(self):
        """åˆ†æé«˜ä»·å€¼å®¢æˆ·"""
        high_value = []
        if self.data is not None:
            # æŒ‰å›½å®¶åˆ†ç»„ï¼Œæ‰¾å‡ºè¯¢ç›˜æ•°é‡å¤šçš„å›½å®¶
            country_counts = self.data['å›½å®¶'].value_counts()
            top_countries = country_counts.head(5)
            
            for country, count in top_countries.items():
                country_data = self.data[self.data['å›½å®¶'] == country]
                # æ‰¾å‡ºè¯¥å›½å®¶çš„ä¸»è¦å®¢æˆ·
                customer_counts = country_data['å®¢æˆ·åç§°'].value_counts()
                top_customer = customer_counts.index[0] if len(customer_counts) > 0 else "æœªçŸ¥"
                high_value.append({
                    'name': top_customer,
                    'country': country,
                    'reason': f"æ¥è‡ªä¸»è¦å¸‚åœºï¼Œè¯¢ç›˜é‡ {count} æ¬¡ (From major market, {count} inquiries)"
                })
        
        return high_value
    
    def _analyze_priority_customers(self):
        """åˆ†æéœ€è¦é‡ç‚¹è·Ÿè¿›çš„å®¢æˆ·"""
        priority = []
        if self.data is not None:
            # æ‰¾å‡ºè·Ÿè¿›ç­‰çº§ä¸ºAçš„å®¢æˆ·
            level_a_customers = self.data[self.data['è·Ÿè¿›ç­‰çº§'] == 'A']
            if not level_a_customers.empty:
                for _, customer in level_a_customers.iterrows():
                    priority.append({
                        'name': customer['å®¢æˆ·åç§°'],
                        'country': customer['å›½å®¶'],
                        'reason': "ç²¾å‡†è¯¢ç›˜ï¼Œé«˜è½¬åŒ–æ½œåŠ› (Precise inquiry, high conversion potential)"
                    })
            
            # æ‰¾å‡ºæœ€è¿‘è¯¢ç›˜ä½†æœªè·Ÿè¿›çš„å®¢æˆ·
            recent_date = pd.Timestamp(datetime.now().date()) - pd.Timedelta(days=7)
            recent_customers = self.data[self.data['è¯¢ç›˜æ—¶é—´'] >= recent_date]
            for _, customer in recent_customers.iterrows():
                if customer['å®¢æˆ·åç§°'] not in [p['name'] for p in priority]:
                    priority.append({
                        'name': customer['å®¢æˆ·åç§°'],
                        'country': customer['å›½å®¶'],
                        'reason': "æœ€è¿‘è¯¢ç›˜ï¼Œéœ€è¦åŠæ—¶è·Ÿè¿› (Recent inquiry, timely follow-up needed)"
                    })
        
        return priority[:10]  # é™åˆ¶æ•°é‡
    
    def _analyze_lost_customers(self):
        """åˆ†æå¤±å•åŸå› """
        lost_reasons = {}
        if self.data is not None and 'å¤‡æ³¨ (å¤±å•åŸå› +è·Ÿè¿›æœºä¼šç‚¹)' in self.data.columns:
            remarks = self.data['å¤‡æ³¨ (å¤±å•åŸå› +è·Ÿè¿›æœºä¼šç‚¹)'].dropna()
            for remark in remarks:
                if isinstance(remark, str) and remark.strip():
                    # ç®€å•çš„å…³é”®è¯åˆ†æ
                    if 'ä¸å›' in remark or 'æœªè¯»' in remark:
                        lost_reasons['å®¢æˆ·ä¸å›å¤'] = lost_reasons.get('å®¢æˆ·ä¸å›å¤', 0) + 1
                    elif 'ä»·æ ¼' in remark:
                        lost_reasons['ä»·æ ¼é—®é¢˜'] = lost_reasons.get('ä»·æ ¼é—®é¢˜', 0) + 1
                    elif 'MOQ' in remark or 'moq' in remark:
                        lost_reasons['èµ·è®¢é‡é—®é¢˜'] = lost_reasons.get('èµ·è®¢é‡é—®é¢˜', 0) + 1
                    elif 'ä¸ªäºº' in remark:
                        lost_reasons['ä¸ªäººä¹°å®¶'] = lost_reasons.get('ä¸ªäººä¹°å®¶', 0) + 1
                    else:
                        lost_reasons['å…¶ä»–åŸå› '] = lost_reasons.get('å…¶ä»–åŸå› ', 0) + 1
        
        return lost_reasons
    
    def _generate_recommendations(self):
        """ç”Ÿæˆå‘å±•å»ºè®®"""
        recommendations = [
            "åŠ å¼ºé«˜ä»·å€¼å¸‚åœºçš„å®¢æˆ·ç»´æŠ¤ï¼Œå»ºç«‹é•¿æœŸåˆä½œå…³ç³» (Strengthen customer maintenance in high-value markets and establish long-term partnerships)",
            "ä¼˜åŒ–ä»·æ ¼ç­–ç•¥ï¼Œæé«˜ç«äº‰åŠ› (Optimize pricing strategy to improve competitiveness)",
            "é™ä½èµ·è®¢é‡è¦æ±‚ï¼Œå¸å¼•æ›´å¤šä¸­å°å®¢æˆ· (Reduce MOQ requirements to attract more small and medium-sized customers)",
            "å»ºç«‹å®¢æˆ·åˆ†çº§ç®¡ç†ä½“ç³»ï¼Œå·®å¼‚åŒ–æœåŠ¡ (Establish customer tiering management system with differentiated services)",
            "åŠ å¼ºäº§å“å®£ä¼ ï¼Œæé«˜å“ç‰ŒçŸ¥ååº¦ (Strengthen product promotion to enhance brand awareness)",
            "å»ºç«‹å®¢æˆ·åé¦ˆæœºåˆ¶ï¼ŒæŒç»­æ”¹è¿›æœåŠ¡è´¨é‡ (Establish customer feedback mechanism to continuously improve service quality)",
            "å¼€å‘æ–°äº§å“çº¿ï¼Œæ‰©å¤§å¸‚åœºè¦†ç›– (Develop new product lines to expand market coverage)",
            "åŠ å¼ºå›¢é˜ŸåŸ¹è®­ï¼Œæé«˜è·Ÿè¿›æ•ˆç‡ (Strengthen team training to improve follow-up efficiency)"
        ]
        return recommendations
    
    def get_smart_alerts(self) -> List[Dict]:
        """è·å–æ™ºèƒ½æé†’ - 6å¤§æ ¸å¿ƒæé†’ç±»åˆ«"""
        if self.data is None:
            raise ValueError("è¯·å…ˆè¯»å–æ•°æ®")
        
        alerts = []
        current_date = datetime.now()
        
        # ç¬¬ä¸€ç±»ï¼šé«˜ä»·å€¼å®¢æˆ·è¯†åˆ«æé†’ï¼ˆæŠ“ä½é»„é‡‘æœºä¼šï¼‰
        alerts.extend(self._check_high_value_customers())
        
        # ç¬¬äºŒç±»ï¼šåƒåœ¾/é’“é±¼/ä½è´¨é‡è¯¢ç›˜é¢„è­¦
        alerts.extend(self._check_low_quality_inquiries())
        
        # ç¬¬ä¸‰ç±»ï¼šé•¿æœŸæœªè·Ÿè¿›æé†’ï¼ˆé˜²æ­¢å®¢æˆ·æµå¤±ï¼‰
        alerts.extend(self._check_long_term_unfollow(current_date))
        
        # ç¬¬å››ç±»ï¼šåŒºåŸŸé›†ä¸­è¶‹åŠ¿æé†’ï¼ˆæŠŠæ¡å¸‚åœºåŠ¨å‘ï¼‰
        alerts.extend(self._check_regional_trends())
        
        # ç¬¬äº”ç±»ï¼šäº§å“çƒ­åº¦å˜åŒ–æé†’ï¼ˆæŒ‡å¯¼å¤‡è´§ä¸æ¨å¹¿ï¼‰
        alerts.extend(self._check_product_trends())
        
        # ç¬¬å…­ç±»ï¼šè½¬åŒ–æ¼æ–—å¼‚å¸¸æé†’ï¼ˆä¼˜åŒ–é”€å”®ç­–ç•¥ï¼‰
        alerts.extend(self._check_conversion_funnel())
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        priority_order = {'high': 0, 'medium': 1, 'low': 2}
        alerts.sort(key=lambda x: priority_order.get(x['priority'], 3))
        
        return alerts
    
    def _check_high_value_customers(self) -> List[Dict]:
        """ç¬¬ä¸€ç±»ï¼šé«˜ä»·å€¼å®¢æˆ·è¯†åˆ«æé†’"""
        alerts = []
        
        if 'å¤‡æ³¨ (å¤±å•åŸå› +è·Ÿè¿›æœºä¼šç‚¹)' not in self.data.columns:
            return alerts
        
        # é«˜ä»·å€¼å…³é”®è¯
        high_value_keywords = [
            'è‡ªæœ‰è®¾è®¡å›¾', 'å®šåˆ¶å“ç‰Œ', 'é¦–å•100ä»¶', 'é¦–å•80ä»¶', 'é¦–å•50ä»¶',
            'å®˜ç½‘', 'çº¿ä¸Šåº—é“º', 'OEM', 'å“ç‰Œå®šåˆ¶', 'å¤§å•', 'é•¿æœŸåˆä½œ',
            'wholesale', 'bulk order', 'brand', 'custom', 'private label'
        ]
        
        for idx, row in self.data.iterrows():
            remark = str(row.get('å¤‡æ³¨ (å¤±å•åŸå› +è·Ÿè¿›æœºä¼šç‚¹)', '')).lower()
            customer_name = row.get('å®¢æˆ·åç§°', 'æœªçŸ¥')
            country = row.get('å›½å®¶', 'æœªçŸ¥')
            
            for keyword in high_value_keywords:
                if keyword.lower() in remark:
                    alerts.append({
                        'type': 'high_value_opportunity',
                        'priority': 'high',
                        'category': 'ğŸš¨ é«˜ä»·å€¼å®¢æˆ·è¯†åˆ«',
                        'message': f"[é«˜æ½œåŠ›å®¢æˆ·] {customer_name} ({country}) - å¤‡æ³¨æåŠ'{keyword}'",
                        'suggestion': 'ç«‹å³ç”µè¯æˆ–TMè”ç³»ï¼Œæä¾›OEMæŠ¥ä»·æ¨¡æ¿ï¼Œå‘é€æˆåŠŸæ¡ˆä¾‹',
                        'customer_name': customer_name,
                        'country': country,
                        'keyword': keyword
                    })
                    break
        
        # æ£€æŸ¥è·Ÿè¿›ç­‰çº§å‡çº§ï¼ˆB/C å‡ä¸º Xï¼‰
        if 'è·Ÿè¿›ç­‰çº§' in self.data.columns:
            x_level_customers = self.data[self.data['è·Ÿè¿›ç­‰çº§'] == 'X']
            for _, customer in x_level_customers.head(5).iterrows():
                alerts.append({
                    'type': 'level_upgraded',
                    'priority': 'high',
                    'category': 'ğŸš¨ é«˜ä»·å€¼å®¢æˆ·è¯†åˆ«',
                    'message': f"[å‡çº§æé†’] {customer.get('å®¢æˆ·åç§°', 'æœªçŸ¥')} ({customer.get('å›½å®¶', 'æœªçŸ¥')}) å·²è¿›å…¥æ ·å“é˜¶æ®µ",
                    'suggestion': 'åˆ›å»ºä»»åŠ¡ï¼šå¯„æ ·+å‘ç¥¨+ç‰©æµå•å·å½•å…¥ï¼Œè®¾ç½®7å¤©åå›è®¿',
                    'customer_name': customer.get('å®¢æˆ·åç§°', 'æœªçŸ¥'),
                    'level': 'X'
                })
        
        return alerts
    
    def _check_low_quality_inquiries(self) -> List[Dict]:
        """ç¬¬äºŒç±»ï¼šåƒåœ¾/é’“é±¼/ä½è´¨é‡è¯¢ç›˜é¢„è­¦"""
        alerts = []
        
        if 'å¤‡æ³¨ (å¤±å•åŸå› +è·Ÿè¿›æœºä¼šç‚¹)' not in self.data.columns:
            return alerts
        
        # ä½è´¨é‡å…³é”®è¯
        low_quality_keywords = [
            'é’“é±¼', 'æ–°æ³¨å†Œç”¨æˆ·æœªè¯»', 'ä¿ƒé”€å•†', 'ä¸€å¥è¯è¯¢ç›˜', 'ä¸å¯¹å£',
            'ä¸ªäººä¹°å®¶', 'åƒåœ¾è¯¢ç›˜', 'æ— æ•ˆè¯¢ç›˜', 'è¯ˆéª—', 'éª—å­'
        ]
        
        low_quality_count = 0
        for idx, row in self.data.iterrows():
            remark = str(row.get('å¤‡æ³¨ (å¤±å•åŸå› +è·Ÿè¿›æœºä¼šç‚¹)', '')).lower()
            customer_name = row.get('å®¢æˆ·åç§°', 'æœªçŸ¥')
            country = row.get('å›½å®¶', 'æœªçŸ¥')
            level = row.get('å®¢æˆ·å±‚çº§', 'L0')
            
            for keyword in low_quality_keywords:
                if keyword.lower() in remark:
                    low_quality_count += 1
                    alerts.append({
                        'type': 'low_quality_warning',
                        'priority': 'low',
                        'category': 'ğŸ›‘ ä½è´¨é‡è¯¢ç›˜é¢„è­¦',
                        'message': f"[ä½è´¨é¢„è­¦] {customer_name} ({country}) - æ ‡è®°ä¸º\"{keyword}\"",
                        'suggestion': 'æ ‡è®°ä¸ºCçº§ï¼Œå½’å…¥è§‚å¯Ÿæ± ï¼Œä¸æŠ•å…¥æ·±åº¦æ²Ÿé€šèµ„æº',
                        'customer_name': customer_name,
                        'country': country
                    })
                    break
        
        # æ·»åŠ æ±‡æ€»æé†’
        if low_quality_count > 0:
            c_level_count = len(self.data[self.data['è·Ÿè¿›ç­‰çº§'] == 'C']) if 'è·Ÿè¿›ç­‰çº§' in self.data.columns else 0
            total_count = len(self.data)
            c_level_percentage = (c_level_count / total_count * 100) if total_count > 0 else 0
            
            alerts.append({
                'type': 'low_quality_summary',
                'priority': 'medium',
                'category': 'ğŸ›‘ ä½è´¨é‡è¯¢ç›˜é¢„è­¦',
                'message': f"[æ•°æ®æ´å¯Ÿ] å½“å‰Cçº§å æ¯”{c_level_percentage:.1f}%ï¼Œæ™ºèƒ½è¿‡æ»¤å¯èŠ‚çœçº¦{c_level_percentage * 0.6:.0f}%æ— æ•ˆæ²Ÿé€šæ—¶é—´",
                'suggestion': f'å‘ç°{low_quality_count}ä¸ªä½è´¨é‡è¯¢ç›˜ï¼Œå»ºè®®ä¼˜åŒ–ç­›é€‰ç­–ç•¥',
                'count': low_quality_count
            })
        
        return alerts
    
    def _check_long_term_unfollow(self, current_date) -> List[Dict]:
        """ç¬¬ä¸‰ç±»ï¼šé•¿æœŸæœªè·Ÿè¿›æé†’ï¼ˆé˜²æ­¢å®¢æˆ·æµå¤±ï¼‰"""
        alerts = []
        
        if 'æœ€åè·Ÿè¿›æ—¶é—´' not in self.data.columns:
            return alerts
        
        # æ£€æŸ¥è¶…è¿‡5å¤©æœªè·Ÿè¿›ä¸”éX/Açº§çš„å®¢æˆ·
        for idx, row in self.data.iterrows():
            last_followup = row.get('æœ€åè·Ÿè¿›æ—¶é—´')
            level = row.get('è·Ÿè¿›ç­‰çº§', '')
            customer_name = row.get('å®¢æˆ·åç§°', 'æœªçŸ¥')
            country = row.get('å›½å®¶', 'æœªçŸ¥')
            
            if pd.notna(last_followup) and level not in ['X', 'A']:
                try:
                    last_date = pd.to_datetime(last_followup)
                    days_overdue = (current_date - last_date).days
                    
                    if days_overdue > 5:
                        alerts.append({
                            'type': 'long_term_unfollow',
                            'priority': 'high' if days_overdue > 7 else 'medium',
                            'category': 'ğŸ” é•¿æœŸæœªè·Ÿè¿›æé†’',
                            'message': f"[æ»ç•™æé†’] {customer_name} ({country}) å·²{days_overdue}å¤©æœªå›å¤",
                            'suggestion': 'é‡æ–°è§¦è¾¾ï¼Œå°è¯•æ¢ä¸»é¢˜é‚®ä»¶æˆ–TMæ¶ˆæ¯: "ä¸Šæ¬¡æåˆ°çš„ä»·æ ¼æ˜¯å¦åˆé€‚ï¼Ÿæˆ‘ä»¬å¯ä»¥è°ƒæ•´MOQæ–¹æ¡ˆã€‚"',
                            'customer_name': customer_name,
                            'days_overdue': days_overdue
                        })
                except:
                    pass
        
        # æ£€æŸ¥å®¢æˆ·æ›¾è¡¨è¾¾å…´è¶£ä½†"æœªè¯»"æ¶ˆæ¯
        if 'å¤‡æ³¨ (å¤±å•åŸå› +è·Ÿè¿›æœºä¼šç‚¹)' in self.data.columns:
            for idx, row in self.data.iterrows():
                remark = str(row.get('å¤‡æ³¨ (å¤±å•åŸå› +è·Ÿè¿›æœºä¼šç‚¹)', '')).lower()
                if 'æœªè¯»' in remark and ('æ ·å“' in remark or 'å…´è¶£' in remark):
                    customer_name = row.get('å®¢æˆ·åç§°', 'æœªçŸ¥')
                    country = row.get('å›½å®¶', 'æœªçŸ¥')
                    alerts.append({
                        'type': 'unread_message',
                        'priority': 'medium',
                        'category': 'ğŸ” é•¿æœŸæœªè·Ÿè¿›æé†’',
                        'message': f"[å”¤é†’æé†’] {customer_name} ({country}) æ›¾å¯»æ±‚æ ·å“ä½†æœªè¯»ä¿¡æ¯",
                        'suggestion': 'ä½¿ç”¨"å…æ ·å“è´¹é—¨æ§›"ä½œä¸ºé’©å­é‡æ–°æ¿€æ´»',
                        'customer_name': customer_name
                    })
        
        return alerts
    
    def _check_regional_trends(self) -> List[Dict]:
        """ç¬¬å››ç±»ï¼šåŒºåŸŸé›†ä¸­è¶‹åŠ¿æé†’ï¼ˆæŠŠæ¡å¸‚åœºåŠ¨å‘ï¼‰"""
        alerts = []
        
        if 'å›½å®¶' not in self.data.columns or 'è¯¢ç›˜æ—¶é—´' not in self.data.columns:
            return alerts
        
        # æ£€æŸ¥è¿‘ä¸¤å‘¨çš„è¯¢ç›˜
        two_weeks_ago = datetime.now() - timedelta(days=14)
        recent_data = self.data[pd.to_datetime(self.data['è¯¢ç›˜æ—¶é—´']) >= two_weeks_ago]
        
        # ç»Ÿè®¡å›½å®¶é¢‘æ¬¡
        country_counts = recent_data['å›½å®¶'].value_counts()
        
        # æ£€æŸ¥åŒä¸€å›½å®¶è¿ç»­å‡ºç°3æ¬¡åŠä»¥ä¸Š
        for country, count in country_counts.items():
            if count >= 3:
                # æŸ¥æ‰¾è¯¥å›½å®¶çš„çƒ­é—¨äº§å“
                country_data = recent_data[recent_data['å›½å®¶'] == country]
                if 'è¯¢ä»·äº§å“' in country_data.columns:
                    product_counts = country_data['è¯¢ä»·äº§å“'].value_counts()
                    top_product = product_counts.index[0] if len(product_counts) > 0 else 'æœªçŸ¥'
                    
                    alerts.append({
                        'type': 'regional_hotspot',
                        'priority': 'high',
                        'category': 'ğŸŒ åŒºåŸŸé›†ä¸­è¶‹åŠ¿',
                        'message': f"[åŒºåŸŸçƒ­ç‚¹] è¿‘ä¸¤å‘¨{country}å‡ºç°{count}æ¬¡è¯¢ç›˜ï¼Œé›†ä¸­åœ¨{top_product}",
                        'suggestion': f'å‡†å¤‡{country}æœ¬åœ°åŒ–æ–‡æ¡ˆï¼›æ£€æŸ¥åº“å­˜ä¸ç‰©æµæ–¹æ¡ˆ',
                        'country': country,
                        'count': count,
                        'product': top_product
                    })
        
        # æ£€æŸ¥å¤§æ´²å®¢æˆ·å’¨è¯¢é‡å‘¨ç¯æ¯”å¢é•¿
        if 'æ‰€å±å¤§æ´²' in self.data.columns:
            one_week_ago = datetime.now() - timedelta(days=7)
            this_week_data = self.data[pd.to_datetime(self.data['è¯¢ç›˜æ—¶é—´']) >= one_week_ago]
            last_week_data = self.data[
                (pd.to_datetime(self.data['è¯¢ç›˜æ—¶é—´']) >= two_weeks_ago) &
                (pd.to_datetime(self.data['è¯¢ç›˜æ—¶é—´']) < one_week_ago)
            ]
            
            this_week_continent = this_week_data['æ‰€å±å¤§æ´²'].value_counts()
            last_week_continent = last_week_data['æ‰€å±å¤§æ´²'].value_counts()
            
            for continent in this_week_continent.index:
                this_count = this_week_continent.get(continent, 0)
                last_count = last_week_continent.get(continent, 0)
                
                if last_count > 0:
                    growth_rate = ((this_count - last_count) / last_count) * 100
                    if growth_rate > 50:
                        alerts.append({
                            'type': 'emerging_market',
                            'priority': 'high',
                            'category': 'ğŸŒ åŒºåŸŸé›†ä¸­è¶‹åŠ¿',
                            'message': f"[æ–°å…´å¸‚åœº] {continent}å®¢æˆ·æ•°é‡æœ¬å‘¨ä¸Šå‡{growth_rate:.0f}%",
                            'suggestion': f'ä¼˜åŒ–{continent}ç‰©æµæ–¹æ¡ˆï¼Œæ›´æ–°è¿è´¹è®¡ç®—å™¨',
                            'continent': continent,
                            'growth_rate': growth_rate
                        })
        
        return alerts
    
    def _check_product_trends(self) -> List[Dict]:
        """ç¬¬äº”ç±»ï¼šäº§å“çƒ­åº¦å˜åŒ–æé†’ï¼ˆæŒ‡å¯¼å¤‡è´§ä¸æ¨å¹¿ï¼‰"""
        alerts = []
        
        if 'è¯¢ä»·äº§å“' not in self.data.columns or 'è¯¢ç›˜æ—¶é—´' not in self.data.columns:
            return alerts
        
        # æ£€æŸ¥è¿‘ä¸€å‘¨çš„äº§å“è¯¢ä»·
        one_week_ago = datetime.now() - timedelta(days=7)
        recent_data = self.data[pd.to_datetime(self.data['è¯¢ç›˜æ—¶é—´']) >= one_week_ago]
        
        product_counts = recent_data['è¯¢ä»·äº§å“'].value_counts()
        
        # æ£€æŸ¥æŸäº§å“è¢«æåŠ â‰¥3æ¬¡/å‘¨
        for product, count in product_counts.items():
            if count >= 3:
                # ç»Ÿè®¡è¯¢é—®è¯¥äº§å“çš„å›½å®¶æ•°
                product_data = recent_data[recent_data['è¯¢ä»·äº§å“'] == product]
                country_count = product_data['å›½å®¶'].nunique()
                
                alerts.append({
                    'type': 'hot_product',
                    'priority': 'high',
                    'category': 'ğŸ§© äº§å“çƒ­åº¦å˜åŒ–',
                    'message': f"[çˆ†æ¬¾é¢„è­¦] {product} è¿‘æœŸè¢«{country_count}ä¸ªå›½å®¶å®¢æˆ·è¯¢é—®{count}æ¬¡",
                    'suggestion': 'ç¡®ä¿è¯¥æ¬¾æ‰“æ ·èµ„æ–™é½å…¨ã€MOQçµæ´»ï¼›ä¸»æ¨æ­¤æ¬¾åšä¸“é¢˜é¡µ',
                    'product': product,
                    'count': count,
                    'country_count': country_count
                })
        
        # æ£€æŸ¥å¤šä¸ªå®¢æˆ·æåŠ"ä½MOQ"éœ€æ±‚
        if 'å¤‡æ³¨ (å¤±å•åŸå› +è·Ÿè¿›æœºä¼šç‚¹)' in self.data.columns:
            low_moq_count = 0
            for idx, row in recent_data.iterrows():
                remark = str(row.get('å¤‡æ³¨ (å¤±å•åŸå› +è·Ÿè¿›æœºä¼šç‚¹)', '')).lower()
                if 'moq' in remark or 'èµ·è®¢é‡' in remark or 'å°æ‰¹é‡' in remark:
                    low_moq_count += 1
            
            if low_moq_count >= 5:
                alerts.append({
                    'type': 'low_moq_demand',
                    'priority': 'medium',
                    'category': 'ğŸ§© äº§å“çƒ­åº¦å˜åŒ–',
                    'message': f"[éœ€æ±‚æ´å¯Ÿ] æœ¬å‘¨æœ‰{low_moq_count}ä½å®¢æˆ·æ˜ç¡®è¡¨ç¤º'ä½MOQ'éœ€æ±‚",
                    'suggestion': 'æ¨å‡º"Mini MOQ Package"æœåŠ¡ï¼ˆå¦‚50ä»¶èµ·è®¢ï¼‰ï¼Œå·®å¼‚åŒ–ç«äº‰',
                    'count': low_moq_count
                })
        
        return alerts
    
    def _check_conversion_funnel(self) -> List[Dict]:
        """ç¬¬å…­ç±»ï¼šè½¬åŒ–æ¼æ–—å¼‚å¸¸æé†’ï¼ˆä¼˜åŒ–é”€å”®ç­–ç•¥ï¼‰"""
        alerts = []
        
        if 'è·Ÿè¿›ç­‰çº§' not in self.data.columns or 'è¯¢ç›˜æ—¶é—´' not in self.data.columns:
            return alerts
        
        # æ£€æŸ¥è¿‘ä¸¤å‘¨çš„æ•°æ®
        two_weeks_ago = datetime.now() - timedelta(days=14)
        recent_data = self.data[pd.to_datetime(self.data['è¯¢ç›˜æ—¶é—´']) >= two_weeks_ago]
        
        if len(recent_data) == 0:
            return alerts
        
        # æ£€æŸ¥Açº§ç²¾å‡†è¯¢ç›˜æ•°é‡
        a_level_count = len(recent_data[recent_data['è·Ÿè¿›ç­‰çº§'] == 'A'])
        
        if a_level_count == 0:
            alerts.append({
                'type': 'no_a_level',
                'priority': 'high',
                'category': 'ğŸ“‰ è½¬åŒ–æ¼æ–—å¼‚å¸¸',
                'message': '[æ¼æ–—è­¦æŠ¥] è¿‡å»14å¤©æ— Açº§ç²¾å‡†è¯¢ç›˜ï¼Œæºå¤´è´¨é‡å¯èƒ½ä¸‹é™',
                'suggestion': 'å¤ç›˜RFQæ ‡é¢˜ä¸äº§å“æè¿°ï¼Œå¢åŠ "Wholesale"ã€"Bulk Order"ç­‰å…³é”®è¯',
                'count': 0
            })
        
        # æ£€æŸ¥Xçº§å®¢æˆ·æ¯”ä¾‹
        x_level_count = len(recent_data[recent_data['è·Ÿè¿›ç­‰çº§'] == 'X'])
        x_level_percentage = (x_level_count / len(recent_data)) * 100
        
        if x_level_percentage < 5:
            alerts.append({
                'type': 'low_conversion',
                'priority': 'high',
                'category': 'ğŸ“‰ è½¬åŒ–æ¼æ–—å¼‚å¸¸',
                'message': f'[è½¬åŒ–ç“¶é¢ˆ] ä»…{x_level_percentage:.1f}%å®¢æˆ·è¿›å…¥æ ·å“é˜¶æ®µï¼Œè½¬åŒ–ç‡åä½',
                'suggestion': 'å¢åŠ å®¢æˆ·è§è¯è§†é¢‘ã€ç¬¬ä¸‰æ–¹æ£€æµ‹æŠ¥å‘Šã€å·¥å‚å®æ‹å¢å¼ºå¯ä¿¡åº¦',
                'percentage': x_level_percentage
            })
        
        # æ£€æŸ¥Cçº§å æ¯”
        c_level_count = len(recent_data[recent_data['è·Ÿè¿›ç­‰çº§'] == 'C'])
        c_level_percentage = (c_level_count / len(recent_data)) * 100
        
        if c_level_percentage > 50:
            alerts.append({
                'type': 'high_c_level',
                'priority': 'medium',
                'category': 'ğŸ“‰ è½¬åŒ–æ¼æ–—å¼‚å¸¸',
                'message': f'[è´¨é‡é¢„è­¦] Cçº§è¯¢ç›˜å æ¯”{c_level_percentage:.0f}%ï¼Œæºå¤´ç­›é€‰éœ€ä¼˜åŒ–',
                'suggestion': 'ä¼˜åŒ–RFQè‡ªåŠ¨å›å¤è§„åˆ™ï¼Œæé«˜åˆç­›é—¨æ§›',
                'percentage': c_level_percentage
            })
        
        return alerts
    
    def export_data(self, output_file: str, format: str = 'excel', group_by_month: bool = True):
        """å¯¼å‡ºæ•°æ® - å®Œå…¨æŒ‰ç…§ç”¨æˆ·æä¾›çš„æ¨¡æ¿æ ¼å¼ï¼ŒæŒ‰æœˆåˆ†ç»„"""
        if self.data is None:
            raise ValueError("è¯·å…ˆè¯»å–æ•°æ®")
        
        os.makedirs(os.path.dirname(output_file) if os.path.dirname(output_file) else '.', exist_ok=True)
        
        if format.lower() == 'excel':
            from openpyxl import Workbook
            from openpyxl.styles import PatternFill, Font, Alignment, Border, Side
            from openpyxl.utils import get_column_letter
            
            wb = Workbook()
            # åˆ é™¤é»˜è®¤å·¥ä½œè¡¨
            wb.remove(wb.active)
            
            # å®šä¹‰åˆ—åå’Œå¯¹åº”çš„é¢œè‰²
            columns_info = {
                'è¯¢ç›˜æ—¶é—´': '0000FF',      # Blue
                'å’¨è¯¢æ–¹å¼': '0000FF',      # Blue
                'è·Ÿè¿›ç­‰çº§': 'FF0000',      # Red
                'å®¢æˆ·åç§°': 'FFFF00',      # Yellow
                'å®¢æˆ·å±‚çº§': 'FFFF00',      # Yellow
                'æ‰€å±å¤§æ´²': 'FF0000',      # Red
                'å›½å®¶': 'FF0000',          # Red
                'è¯¢ä»·äº§å“': 'FF0000',      # Red
                'äº§å“ID': 'FF0000',        # Red
                'è·Ÿè¿›äºº': '0000FF',        # Blue
                'å¤‡æ³¨ (å¤±å•åŸå› +è·Ÿè¿›æœºä¼šç‚¹)': '0000FF',  # Blue
                'æœ€åè·Ÿè¿›æ—¶é—´': '0000FF'    # Blue
            }
            
            # è®¾ç½®è¾¹æ¡†æ ·å¼
            thin_border = Border(
                left=Side(style='thin'),
                right=Side(style='thin'),
                top=Side(style='thin'),
                bottom=Side(style='thin')
            )
            
            if group_by_month and 'è¯¢ç›˜æ—¶é—´' in self.data.columns:
                # æŒ‰æœˆä»½åˆ†ç»„
                def extract_month(date_str):
                    try:
                        date_obj = pd.to_datetime(date_str, errors='coerce')
                        if pd.notna(date_obj):
                            return date_obj.strftime('%Yå¹´%mæœˆ')
                        return 'æœªçŸ¥'
                    except:
                        return 'æœªçŸ¥'
                
                self.data['_æœˆä»½'] = self.data['è¯¢ç›˜æ—¶é—´'].apply(extract_month)
                month_groups = self.data.groupby('_æœˆä»½')
                
                for month, month_data in month_groups:
                    # åˆ›å»ºæœˆåº¦å·¥ä½œè¡¨
                    ws = wb.create_sheet(title=month)
                    
                    # å†™å…¥è¡¨å¤´
                    self._write_header(ws, columns_info, thin_border)
                    
                    # å†™å…¥æ•°æ®
                    self._write_data(ws, month_data, thin_border)
                    
                    # æ·»åŠ è‡ªåŠ¨ç­›é€‰å’Œå†»ç»“çª—æ ¼
                    self._apply_excel_features(ws, len(month_data))
                    
                    # æ·»åŠ åˆ†ç±»è§„åˆ™
                    self._add_classification_rules(ws, len(self.standard_columns))
                    
                    self.logger.info(f"å·²åˆ›å»ºå·¥ä½œè¡¨ '{month}'ï¼ŒåŒ…å« {len(month_data)} æ¡è®°å½•")
                    
                # ç§»é™¤è¾…åŠ©åˆ—
                self.data = self.data.drop(columns=['_æœˆä»½'])
            else:
                # ä¸åˆ†ç»„ï¼Œæ‰€æœ‰æ•°æ®åœ¨ä¸€ä¸ªå·¥ä½œè¡¨
                ws = wb.create_sheet(title="å®¢æˆ·è·Ÿè¿›è¡¨")
                
                # å†™å…¥è¡¨å¤´
                self._write_header(ws, columns_info, thin_border)
                
                # å†™å…¥æ•°æ®
                self._write_data(ws, self.data, thin_border)
                
                # æ·»åŠ è‡ªåŠ¨ç­›é€‰å’Œå†»ç»“çª—æ ¼
                self._apply_excel_features(ws, len(self.data))
                
                # æ·»åŠ åˆ†ç±»è§„åˆ™
                self._write_classification_rules(ws, len(self.standard_columns))
            
            # æ·»åŠ å›¾è¡¨å·¥ä½œè¡¨
            self._add_charts_to_excel(wb, output_file)
            
            # ä¿å­˜æ–‡ä»¶
            wb.save(output_file)
            
        elif format.lower() == 'csv':
            self.data.to_csv(output_file, index=False, encoding='utf-8-sig')
        else:
            raise ValueError("ä¸æ”¯æŒçš„å¯¼å‡ºæ ¼å¼")
        
        self.logger.info(f"æ•°æ®å·²å¯¼å‡ºåˆ°: {output_file}")
    
    def _write_header(self, ws, columns_info, thin_border):
        """å†™å…¥è¡¨å¤´å¹¶è®¾ç½®æ ·å¼"""
        from openpyxl.utils import get_column_letter
        from openpyxl.styles import PatternFill, Font, Alignment, Border
        
        for col_idx, col_name in enumerate(self.standard_columns, 1):
            cell = ws.cell(row=1, column=col_idx, value=col_name)
            
            # è®¾ç½®èƒŒæ™¯é¢œè‰²
            hex_color = columns_info.get(col_name, 'FFFFFF')
            cell.fill = PatternFill(start_color=hex_color, end_color=hex_color, fill_type="solid")
            
            # è®¾ç½®å­—ä½“ï¼šç™½è‰²ã€åŠ ç²—
            cell.font = Font(bold=True, color="FFFFFF", size=11)
            
            # å±…ä¸­
            cell.alignment = Alignment(horizontal='center', vertical='center')
            
            # è®¾ç½®è¾¹æ¡†
            cell.border = thin_border
            
            # è®¾ç½®åˆ—å®½ï¼ˆæ ¹æ®å†…å®¹è°ƒæ•´ï¼‰
            if col_name == 'å¤‡æ³¨ (å¤±å•åŸå› +è·Ÿè¿›æœºä¼šç‚¹)':
                ws.column_dimensions[get_column_letter(col_idx)].width = 40
            elif col_name in ['å®¢æˆ·åç§°', 'è¯¢ä»·äº§å“']:
                ws.column_dimensions[get_column_letter(col_idx)].width = 20
            elif col_name in ['è¯¢ç›˜æ—¶é—´', 'æœ€åè·Ÿè¿›æ—¶é—´']:
                ws.column_dimensions[get_column_letter(col_idx)].width = 12
            else:
                ws.column_dimensions[get_column_letter(col_idx)].width = 15
        
        # è®¾ç½®è¡¨å¤´è¡Œé«˜
        ws.row_dimensions[1].height = 25
    
    def _apply_excel_features(self, ws, data_rows):
        """åº”ç”¨Excelç‰¹æ€§ï¼šè‡ªåŠ¨ç­›é€‰ã€å†»ç»“çª—æ ¼ã€éš”è¡Œå¡«å……"""
        from openpyxl.utils import get_column_letter
        from openpyxl.styles import PatternFill
        
        # 1. æ·»åŠ è‡ªåŠ¨ç­›é€‰ï¼ˆä»A1åˆ°æœ€åä¸€åˆ—çš„æœ€åä¸€è¡Œï¼‰
        last_col = get_column_letter(len(self.standard_columns))
        last_row = data_rows + 1  # è¡¨å¤´å 1è¡Œ
        ws.auto_filter.ref = f'A1:{last_col}{last_row}'
        
        # 2. å†»ç»“é¦–è¡Œï¼ˆè¡¨å¤´ï¼‰
        ws.freeze_panes = 'A2'
        
        # 3. éš”è¡Œå¡«å……æµ…ç°è‰²ï¼ˆæé«˜å¯è¯»æ€§ï¼‰
        light_gray_fill = PatternFill(start_color='F2F2F2', end_color='F2F2F2', fill_type='solid')
        for row_idx in range(3, last_row + 1, 2):  # ä»ç¬¬3è¡Œå¼€å§‹ï¼Œæ¯éš”ä¸€è¡Œ
            for col_idx in range(1, len(self.standard_columns) + 1):
                cell = ws.cell(row=row_idx, column=col_idx)
                # åªåœ¨æ²¡æœ‰ç‰¹æ®ŠèƒŒæ™¯è‰²çš„å•å…ƒæ ¼ä¸Šåº”ç”¨
                if not cell.fill or cell.fill.start_color.rgb == '00000000':
                    cell.fill = light_gray_fill
        
        # 4. è®¾ç½®æ‰“å°é€‰é¡¹ï¼ˆå¯é€‰ï¼‰
        ws.page_setup.orientation = ws.ORIENTATION_LANDSCAPE  # æ¨ªå‘
        ws.page_setup.fitToPage = True
        ws.page_setup.fitToHeight = False
        ws.page_setup.fitToWidth = 1
        
        # 5. è®¾ç½®ç¼©æ”¾ï¼ˆå¯é€‰ï¼Œé»˜è®¤100%ï¼‰
        ws.sheet_view.zoomScale = 100
        
        self.logger.info(f"å·²åº”ç”¨Excelç‰¹æ€§ï¼šè‡ªåŠ¨ç­›é€‰èŒƒå›´ A1:{last_col}{last_row}ï¼Œå†»ç»“é¦–è¡Œ")
    
    def _write_data(self, ws, data, thin_border):
        """å†™å…¥æ•°æ®è¡Œ"""
        from openpyxl.styles import Font
        from openpyxl.cell.cell import TYPE_STRING
        from openpyxl.utils import get_column_letter
        
        for row_idx, (_, row_data) in enumerate(data.iterrows(), 2):
            for col_idx, col_name in enumerate(self.standard_columns, 1):
                cell_value = row_data.get(col_name, '')
                
                # ç‰¹æ®Šå¤„ç†è¯¢ç›˜æ—¶é—´ï¼Œåªæ˜¾ç¤ºæ—¥æœŸä¸æ˜¾ç¤ºæ—¶é—´
                if col_name == 'è¯¢ç›˜æ—¶é—´':
                    if pd.notna(cell_value) and cell_value != '':
                        try:
                            if isinstance(cell_value, pd.Timestamp):
                                cell_value = cell_value.strftime('%Y-%m-%d')
                            elif isinstance(cell_value, str):
                                # å°è¯•è§£æå­—ç¬¦ä¸²å¹¶åªå–æ—¥æœŸéƒ¨åˆ†
                                date_obj = pd.to_datetime(cell_value, errors='coerce')
                                if pd.notna(date_obj):
                                    cell_value = date_obj.strftime('%Y-%m-%d')
                        except Exception:
                            pass
                
                cell = ws.cell(row=row_idx, column=col_idx, value=cell_value)
                
                # è®¾ç½®è¾¹æ¡†
                cell.border = thin_border
                
                # è®¾ç½®å¯¹é½æ–¹å¼
                from openpyxl.styles import Alignment
                if col_name == 'å¤‡æ³¨ (å¤±å•åŸå› +è·Ÿè¿›æœºä¼šç‚¹)':
                    # å¤‡æ³¨åˆ—ï¼šå·¦å¯¹é½ï¼Œè‡ªåŠ¨æ¢è¡Œ
                    cell.alignment = Alignment(horizontal='left', vertical='top', wrap_text=True)
                elif col_name in ['è¯¢ç›˜æ—¶é—´', 'æœ€åè·Ÿè¿›æ—¶é—´', 'è·Ÿè¿›ç­‰çº§', 'å®¢æˆ·å±‚çº§', 'å’¨è¯¢æ–¹å¼']:
                    # æ—¶é—´å’Œç­‰çº§åˆ—ï¼šå±…ä¸­å¯¹é½
                    cell.alignment = Alignment(horizontal='center', vertical='center')
                else:
                    # å…¶ä»–åˆ—ï¼šå·¦å¯¹é½
                    cell.alignment = Alignment(horizontal='left', vertical='center')
                
                # äº§å“IDå¿…é¡»è®¾ä¸ºæ–‡æœ¬æ ¼å¼ï¼ˆé˜²æ­¢æ˜¾ç¤ºä¸ºç§‘å­¦è®¡æ•°æ³•ï¼‰
                if col_name == 'äº§å“ID':
                    cell.data_type = TYPE_STRING
                    # å¦‚æœæ˜¯æ•°å­—ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²
                    if isinstance(cell_value, (int, float)) and not pd.isna(cell_value):
                        cell.value = str(int(cell_value))  # è½¬æ¢ä¸ºæ•´æ•°å†è½¬å­—ç¬¦ä¸²ï¼Œä¿æŒåŸå§‹æ ¼å¼
                
                # æœ€åè·Ÿè¿›æ—¶é—´ä¿æŒåŸå§‹æ ¼å¼ï¼ˆåªæ˜¾ç¤ºæ—¥æœŸï¼‰
                if col_name == 'æœ€åè·Ÿè¿›æ—¶é—´':
                    # å¦‚æœæ˜¯datetimeç±»å‹ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼ˆåªæ˜¾ç¤ºæ—¥æœŸï¼‰
                    if pd.notna(cell_value) and cell_value != '':
                        try:
                            if isinstance(cell_value, pd.Timestamp):
                                cell.value = cell_value.strftime('%Y-%m-%d')
                            elif isinstance(cell_value, str):
                                # å°è¯•è§£æå­—ç¬¦ä¸²å¹¶åªå–æ—¥æœŸéƒ¨åˆ†
                                date_obj = pd.to_datetime(cell_value, errors='coerce')
                                if pd.notna(date_obj):
                                    cell.value = date_obj.strftime('%Y-%m-%d')
                                else:
                                    cell.value = cell_value
                        except Exception:
                            pass
                
                # å¦‚æœæ˜¯æŒ‡å®šåˆ—ï¼Œè®¾ç½®çº¢è‰²å­—ä½“
                if col_name in ['è·Ÿè¿›ç­‰çº§', 'æ‰€å±å¤§æ´²', 'å›½å®¶', 'è¯¢ä»·äº§å“', 'äº§å“ID']:
                    cell.font = Font(color="FF0000")
    
    def _add_classification_rules(self, ws, standard_col_count):
        """æ·»åŠ åˆ†ç±»è§„åˆ™è¯´æ˜"""
        self._write_classification_rules(ws, standard_col_count)
    
    def _write_classification_rules(self, ws, standard_col_count):
        """å†™å…¥åˆ†ç±»è§„åˆ™è¯´æ˜"""
        from openpyxl.styles import Font, Alignment
        from openpyxl.utils import get_column_letter
        
        # åˆ†ç±»è§„åˆ™
        classification_rules = [
            ("A: ç²¾å‡†è¯¢ç›˜", "å®¢æˆ·æ˜ç¡®æŒ‡å‡ºäº§å“éœ€æ±‚, åŒ…å«å„ç§ä¿¡æ¯ (æ•°é‡ã€è¿è¾“/æ”¯ä»˜è¦æ±‚ã€å…¬å¸ä¿¡æ¯ç­‰)"),
            ("B: æ™®é€šè¯¢ç›˜", "å¹¿æ’’ç½‘è¯¢ç›˜, å†…å®¹å¹¿æ³›, åªæ˜¯è¯¢ä»·æˆ–å‘å¯¹äº§å“æ„Ÿå…´è¶£, æˆ–ä¿¡æ¯æœªè¯», éœ€è¦ç»§ç»­è·Ÿè¿›äº†"),
            ("C: ä¸ªäººä¹°å®¶/ä¸åŒ¹é…è¯¢ç›˜/åƒåœ¾è¯¢ç›˜", ""),
            ("X: å·²ä¸‹æ ·å“å•/å¤§è´§å®¢æˆ·, æŒç»­è·Ÿè¿›", "")
        ]
        
        start_col_rules = standard_col_count + 2
        
        # å†™å…¥è§„åˆ™æ ‡é¢˜
        ws.cell(row=1, column=start_col_rules, value="è·Ÿè¿›ç­‰çº§åˆ†ç±»è¯´æ˜").font = Font(bold=True)
        ws.cell(row=1, column=start_col_rules).alignment = Alignment(horizontal='center')
        
        # å†™å…¥è§„åˆ™å†…å®¹
        for idx, (level, desc) in enumerate(classification_rules, 2):
            ws.cell(row=idx, column=start_col_rules, value=level).font = Font(bold=True)
            if desc:
                ws.cell(row=idx, column=start_col_rules + 1, value=desc)
        
        # è°ƒæ•´è§„åˆ™è¯´æ˜çš„åˆ—å®½
        ws.column_dimensions[get_column_letter(start_col_rules)].width = 25
        ws.column_dimensions[get_column_letter(start_col_rules + 1)].width = 50
    
    def _add_charts_to_excel(self, wb, output_file):
        """æ·»åŠ å›¾è¡¨åˆ°Excelå·¥ä½œè¡¨"""
        from openpyxl.drawing.image import Image as XLImage
        from openpyxl.styles import Font
        import os
        
        # å…ˆç”Ÿæˆæ‰€æœ‰å›¾è¡¨åˆ°outputç›®å½•
        chart_dir = './output'
        try:
            # ç”Ÿæˆå›¾è¡¨ï¼ˆå¦‚æœè¿˜æ²¡æœ‰ç”Ÿæˆï¼‰
            if not os.path.exists(os.path.join(chart_dir, 'country_dist.png')):
                self.logger.info("æ­£åœ¨ç”Ÿæˆå›¾è¡¨...")
                self.generate_visualizations(chart_dir)
            
            # åˆ›å»ºå›¾è¡¨å·¥ä½œè¡¨
            ws_charts = wb.create_sheet(title="ğŸ“Šæ•°æ®å›¾è¡¨")
            
            # å›¾è¡¨æ–‡ä»¶åˆ—è¡¨ï¼ˆå¯¹åº”generate_visualizationsç”Ÿæˆçš„æ–‡ä»¶åï¼‰
            charts_info = [
                {'file': 'country_distribution.png', 'title': 'å›½å®¶åˆ†å¸ƒTOP15'},
                {'file': 'follow_up_distribution.png', 'title': 'è·Ÿè¿›ç­‰çº§åˆ†å¸ƒ'},
                {'file': 'product_popularity.png', 'title': 'äº§å“åˆ†å¸ƒTOP10'},
                {'file': 'daily_trend.png', 'title': 'æ¯æ—¥è¯¢ç›˜è¶‹åŠ¿'},
                {'file': 'continent_distribution.png', 'title': 'å¤§æ´²åˆ†å¸ƒ'},
            ]
            
            current_row = 2
            for chart_info in charts_info:
                chart_file = os.path.join(chart_dir, chart_info['file'])
                
                if os.path.exists(chart_file):
                    # æ·»åŠ æ ‡é¢˜
                    title_cell = ws_charts.cell(row=current_row, column=2)
                    title_cell.value = chart_info['title']
                    title_cell.font = Font(name='å¾®è½¯é›…é»‘', size=14, bold=True, color='0066CC')
                    
                    # æ’å…¥å›¾ç‰‡
                    try:
                        img = XLImage(chart_file)
                        # è°ƒæ•´å›¾ç‰‡å¤§å°
                        img.width = 600
                        img.height = 400
                        
                        # è®¡ç®—æ’å…¥ä½ç½®ï¼ˆæ ‡é¢˜ä¸‹æ–¹ï¼‰
                        img_position = f'B{current_row + 1}'
                        ws_charts.add_image(img, img_position)
                        
                        self.logger.info(f"[OK] å›¾è¡¨ '{chart_info['title']}' å·²æ·»åŠ åˆ°Excel")
                        
                        # æ›´æ–°è¡Œä½ç½®ï¼ˆå›¾ç‰‡é«˜åº¦çº¦20è¡Œ + 3è¡Œé—´è·ï¼‰
                        current_row += 23
                        
                    except Exception as e:
                        self.logger.warning(f"æ— æ³•æ·»åŠ å›¾è¡¨ {chart_file}: {e}")
                        current_row += 3
                else:
                    self.logger.warning(f"å›¾è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {chart_file}")
            
            # è°ƒæ•´åˆ—å®½
            ws_charts.column_dimensions['B'].width = 80
            
            self.logger.info("[OK] æ‰€æœ‰å›¾è¡¨å·²æ·»åŠ åˆ°Excelçš„'ğŸ“Šæ•°æ®å›¾è¡¨'å·¥ä½œè¡¨")
            
        except Exception as e:
            self.logger.error(f"æ·»åŠ å›¾è¡¨åˆ°Excelæ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            # ä¸å½±å“ä¸»è¦çš„æ•°æ®å¯¼å‡ºåŠŸèƒ½


def main():
    """å‘½ä»¤è¡Œæ¥å£"""
    parser = argparse.ArgumentParser(description='é˜¿é‡Œå›½é™…ä¸šåŠ¡æ™ºèƒ½å¤ç›˜å·¥å…·')
    parser.add_argument('--import', dest='import_file', help='å¯¼å…¥Excelæ–‡ä»¶')
    parser.add_argument('--export', dest='export_file', help='å¯¼å‡ºæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--analyze', action='store_true', help='æ‰§è¡Œæ•°æ®åˆ†æ')
    parser.add_argument('--report', dest='report_file', help='ç”ŸæˆæŠ¥å‘Šæ–‡ä»¶')
    parser.add_argument('--visualize', action='store_true', help='ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨')
    parser.add_argument('--alerts', action='store_true', help='æ˜¾ç¤ºæ™ºèƒ½æé†’')
    parser.add_argument('--config', default='config.json', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--gui', action='store_true', help='å¯åŠ¨GUIç•Œé¢')
    
    args = parser.parse_args()
    
    # åˆ›å»ºåˆ†æå™¨å®ä¾‹
    analyzer = AliBusinessAnalyzer(args.config)
    
    if args.gui:
        # å¯åŠ¨GUIç•Œé¢
        from ali_business_gui import AliBusinessGUI
        app = AliBusinessGUI(analyzer)
        app.run()
    else:
        # å‘½ä»¤è¡Œæ¨¡å¼
        try:
            if args.import_file:
                analyzer.read_excel(args.import_file)
                print(f"æˆåŠŸå¯¼å…¥æ•°æ®: {len(analyzer.data)} æ¡è®°å½•")
            
            if args.analyze:
                results = analyzer.analyze_data()
                print("æ•°æ®åˆ†æå®Œæˆ")
                print(f"æ€»å®¢æˆ·æ•°: {results['basic_stats']['total_customers']}")
            
            if args.visualize:
                analyzer.generate_visualizations()
                print("å¯è§†åŒ–å›¾è¡¨å·²ç”Ÿæˆ")
            
            if args.report_file:
                report_file = analyzer.generate_report(args.report_file)
                print(f"æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
            
            if args.alerts:
                alerts = analyzer.get_smart_alerts()
                print(f"å‘ç° {len(alerts)} ä¸ªæé†’:")
                for alert in alerts:
                    print(f"- {alert['message']}")
            
            if args.export_file:
                analyzer.export_data(args.export_file)
                print(f"æ•°æ®å·²å¯¼å‡º: {args.export_file}")
                
        except Exception as e:
            print(f"æ‰§è¡Œå¤±è´¥: {e}")
            analyzer.logger.error(f"æ‰§è¡Œå¤±è´¥: {e}")


if __name__ == "__main__":
    main()
